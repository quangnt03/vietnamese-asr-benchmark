{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PhoWhisper Model Evaluation - Google Colab Standalone\n",
    "\n",
    "This notebook evaluates **5 PhoWhisper models** on Vietnamese datasets.\n",
    "\n",
    "## Models Evaluated:\n",
    "- vinai/PhoWhisper-tiny\n",
    "- vinai/PhoWhisper-base\n",
    "- vinai/PhoWhisper-small\n",
    "- vinai/PhoWhisper-medium\n",
    "- vinai/PhoWhisper-large\n",
    "\n",
    "## Features:\n",
    "- Complete standalone execution (no external files needed)\n",
    "- Downloads datasets from HuggingFace automatically\n",
    "- Calculates comprehensive metrics (WER, CER, MER, WIL, WIP, SER, RTF)\n",
    "- Generates visualizations\n",
    "- Exports results as CSV for cross-model comparison\n",
    "\n",
    "**Runtime**: GPU recommended (T4 or better)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SETUP] Installing required packages...\n",
      "[OK] All packages installed successfully!\n",
      "\n",
      "[INFO] CUDA available: True\n",
      "[INFO] GPU: NVIDIA GeForce RTX 3050 Laptop GPU\n"
     ]
    }
   ],
   "source": [
    "print('[SETUP] Installing required packages...')\n",
    "!pip install -q transformers==4.57.1 torch torchcodec torchaudio librosa soundfile jiwer datasets accelerate pandas matplotlib seaborn scipy tqdm\n",
    "print('[OK] All packages installed successfully!')\n",
    "\n",
    "# Check GPU availability\n",
    "import torch\n",
    "print(f'\\n[INFO] CUDA available: {torch.cuda.is_available()}')\n",
    "if torch.cuda.is_available():\n",
    "    print(f'[INFO] GPU: {torch.cuda.get_device_name(0)}')\n",
    "else:\n",
    "    print('[WARNING] Running on CPU - evaluation will be slower')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Embedded Helper Functions\n",
    "\n",
    "All necessary code from the Vietnamese ASR framework is embedded below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pc/miniconda3/envs/lactech-stt/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[OK] All helper functions loaded successfully!\n"
     ]
    }
   ],
   "source": [
    "# Embedded Vietnamese ASR Evaluation Code\n",
    "# This cell contains all necessary functions from src/ modules\n",
    "\n",
    "import time\n",
    "import warnings\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from dataclasses import dataclass\n",
    "from typing import List, Dict, Optional\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# ============================================================================\n",
    "# METRICS MODULE\n",
    "# ============================================================================\n",
    "\n",
    "from jiwer import wer, cer, mer, wil, wip, process_words\n",
    "\n",
    "class ASRMetrics:\n",
    "    \"\"\"Metrics calculator for ASR evaluation.\"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def calculate_all_metrics(references: List[str], hypotheses: List[str]) -> Dict:\n",
    "        \"\"\"Calculate all metrics for batch of utterances.\"\"\"\n",
    "        # Join all references and hypotheses\n",
    "        ref_text = ' '.join(references)\n",
    "        hyp_text = ' '.join(hypotheses)\n",
    "        \n",
    "        # Calculate error details\n",
    "        output = process_words(ref_text, hyp_text)\n",
    "        \n",
    "        return {\n",
    "            'wer': wer(ref_text, hyp_text),\n",
    "            'cer': cer(ref_text, hyp_text),\n",
    "            'mer': mer(ref_text, hyp_text),\n",
    "            'wil': wil(ref_text, hyp_text),\n",
    "            'wip': wip(ref_text, hyp_text),\n",
    "            'ser': sum(1 for r, h in zip(references, hypotheses) if r != h) / len(references),\n",
    "            'insertions': output.insertions,\n",
    "            'deletions': output.deletions,\n",
    "            'substitutions': output.substitutions\n",
    "        }\n",
    "\n",
    "class RTFTimer:\n",
    "    \"\"\"Context manager for measuring Real-Time Factor.\"\"\"\n",
    "    def __init__(self):\n",
    "        self.start_time = None\n",
    "        self.elapsed_time = None\n",
    "    \n",
    "    def __enter__(self):\n",
    "        self.start_time = time.time()\n",
    "        return self\n",
    "    \n",
    "    def __exit__(self, exc_type, exc_val, exc_tb):\n",
    "        self.elapsed_time = time.time() - self.start_time\n",
    "\n",
    "# ============================================================================\n",
    "# DATASET LOADER MODULE\n",
    "# ============================================================================\n",
    "\n",
    "from datasets import load_dataset, Audio\n",
    "import soundfile as sf\n",
    "import tempfile\n",
    "\n",
    "@dataclass\n",
    "class AudioSample:\n",
    "    \"\"\"Data structure for audio samples.\"\"\"\n",
    "    audio_path: str\n",
    "    transcription: str\n",
    "    duration: float = 0.0\n",
    "    sample_rate: int = 16000\n",
    "    dataset: str = ''\n",
    "    split: str = ''\n",
    "    speaker_id: Optional[str] = None\n",
    "    metadata: Optional[Dict] = None\n",
    "\n",
    "def load_huggingface_dataset(dataset_name: str, max_samples: int = None) -> Dict[str, List[AudioSample]]:\n",
    "    \"\"\"\n",
    "    Load dataset from HuggingFace Hub.\n",
    "    \n",
    "    Supported datasets:\n",
    "    - ViMD: nguyendv02/ViMD_Dataset\n",
    "    - BUD500: linhtran92/viet_bud500\n",
    "    - LSVSC: doof-ferb/LSVSC\n",
    "    - VLSP2020: doof-ferb/vlsp2020_vinai_100h\n",
    "    - VietMed: leduckhai/VietMed\n",
    "    \"\"\"\n",
    "    # Dataset configurations\n",
    "    configs = {\n",
    "        'ViMD': {'id': 'nguyendv02/ViMD_Dataset', 'splits': ['train', 'test', 'valid'], 'audio_col': 'audio', 'text_col': 'text'},\n",
    "        'BUD500': {'id': 'linhtran92/viet_bud500', 'splits': ['train', 'validation', 'test'], 'audio_col': 'audio', 'text_col': 'transcription'},\n",
    "        'LSVSC': {'id': 'doof-ferb/LSVSC', 'splits': ['train', 'validation', 'test'], 'audio_col': 'audio', 'text_col': 'transcription'},\n",
    "        'VLSP2020': {'id': 'doof-ferb/vlsp2020_vinai_100h', 'splits': ['train'], 'audio_col': 'audio', 'text_col': 'transcription'},\n",
    "        'VietMed': {'id': 'leduckhai/VietMed', 'splits': ['train', 'test', 'dev'], 'audio_col': 'audio', 'text_col': 'text'}\n",
    "    }\n",
    "    \n",
    "    if dataset_name not in configs:\n",
    "        raise ValueError(f\"Unknown dataset: {dataset_name}. Available: {list(configs.keys())}\")\n",
    "    \n",
    "    config = configs[dataset_name]\n",
    "    print(f\"[INFO] Loading {dataset_name} from HuggingFace Hub...\")\n",
    "    \n",
    "    samples_by_split = {'train': [], 'val': [], 'test': []}\n",
    "    temp_dir = Path(tempfile.gettempdir()) / 'asr_audio' / dataset_name\n",
    "    temp_dir.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    for split in config['splits']:\n",
    "        try:\n",
    "            # Load split\n",
    "            print(f\"  Loading {split} split...\")\n",
    "            dataset = load_dataset(config['id'], split=split, trust_remote_code=True)\n",
    "            \n",
    "            # Cast audio to 16kHz\n",
    "            if config['audio_col'] in dataset.column_names:\n",
    "                dataset = dataset.cast_column(config['audio_col'], Audio(sampling_rate=16000))\n",
    "            \n",
    "            # Limit samples if specified\n",
    "            if max_samples and len(dataset) > max_samples:\n",
    "                dataset = dataset.select(range(max_samples))\n",
    "            \n",
    "            # Convert to AudioSample objects\n",
    "            samples = []\n",
    "            for idx, item in enumerate(tqdm(dataset, desc=f\"{split}\", leave=False)):\n",
    "                try:\n",
    "                    audio_data = item[config['audio_col']]\n",
    "                    \n",
    "                    # Save audio to temporary file\n",
    "                    audio_path = str(temp_dir / f\"{split}_{idx}.wav\")\n",
    "                    sf.write(audio_path, audio_data['array'], audio_data['sampling_rate'])\n",
    "                    \n",
    "                    # Get duration\n",
    "                    duration = len(audio_data['array']) / audio_data['sampling_rate']\n",
    "                    \n",
    "                    # Get transcription\n",
    "                    transcription = str(item[config['text_col']]).strip().lower()\n",
    "                    \n",
    "                    sample = AudioSample(\n",
    "                        audio_path=audio_path,\n",
    "                        transcription=transcription,\n",
    "                        duration=duration,\n",
    "                        sample_rate=audio_data['sampling_rate'],\n",
    "                        dataset=dataset_name,\n",
    "                        split=split\n",
    "                    )\n",
    "                    samples.append(sample)\n",
    "                except Exception as e:\n",
    "                    print(f\"    [WARNING] Failed to process sample {idx}: {e}\")\n",
    "                    continue\n",
    "            \n",
    "            # Map to standard split names\n",
    "            if split in ['train', 'training']:\n",
    "                samples_by_split['train'].extend(samples)\n",
    "            elif split in ['val', 'validation', 'dev', 'valid']:\n",
    "                samples_by_split['val'].extend(samples)\n",
    "            elif split in ['test', 'testing']:\n",
    "                samples_by_split['test'].extend(samples)\n",
    "            \n",
    "            print(f\"  [OK] Loaded {len(samples)} samples from {split}\")\n",
    "        except Exception as e:\n",
    "            print(f\"  [WARNING] Failed to load {split}: {e}\")\n",
    "    \n",
    "    return samples_by_split\n",
    "\n",
    "# ============================================================================\n",
    "# MODEL EVALUATOR MODULE\n",
    "# ============================================================================\n",
    "\n",
    "from transformers import WhisperProcessor, WhisperForConditionalGeneration\n",
    "import librosa\n",
    "\n",
    "class PhoWhisperModel:\n",
    "    \"\"\"PhoWhisper model wrapper.\"\"\"\n",
    "    \n",
    "    def __init__(self, model_id: str):\n",
    "        self.model_id = model_id\n",
    "        self.device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "        self.processor = None\n",
    "        self.model = None\n",
    "    \n",
    "    def load_model(self):\n",
    "        \"\"\"Load model from HuggingFace.\"\"\"\n",
    "        print(f\"[INFO] Loading {self.model_id}...\")\n",
    "        try:\n",
    "            self.processor = WhisperProcessor.from_pretrained(self.model_id)\n",
    "            self.model = WhisperForConditionalGeneration.from_pretrained(self.model_id)\n",
    "            self.model.to(self.device)\n",
    "            self.model.eval()\n",
    "            print(f\"[OK] Model loaded on {self.device}\")\n",
    "        except Exception as e:\n",
    "            print(f\"[ERROR] Failed to load model: {e}\")\n",
    "            raise\n",
    "    \n",
    "    def transcribe(self, audio_path: str) -> str:\n",
    "        \"\"\"Transcribe audio file.\"\"\"\n",
    "        try:\n",
    "            # Load audio\n",
    "            audio, sr = librosa.load(audio_path, sr=16000)\n",
    "            \n",
    "            # Process audio\n",
    "            input_features = self.processor(\n",
    "                audio,\n",
    "                sampling_rate=16000,\n",
    "                return_tensors=\"pt\"\n",
    "            ).input_features.to(self.device)\n",
    "            \n",
    "            # Generate transcription\n",
    "            with torch.no_grad():\n",
    "                predicted_ids = self.model.generate(input_features)\n",
    "            \n",
    "            transcription = self.processor.batch_decode(\n",
    "                predicted_ids,\n",
    "                skip_special_tokens=True\n",
    "            )[0]\n",
    "            \n",
    "            return transcription.strip().lower()\n",
    "        except Exception as e:\n",
    "            print(f\"[ERROR] Transcription failed: {e}\")\n",
    "            return \"\"\n",
    "\n",
    "print('[OK] All helper functions loaded successfully!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CONFIG] Models to evaluate: 3\n",
      "[CONFIG] Datasets to evaluate: 1\n",
      "[CONFIG] Max samples per split: 50\n",
      "[CONFIG] Output file: /content/phowhisper_results_20251102_213332.csv\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "# Model configuration - Select models to evaluate\n",
    "MODELS_TO_TEST = [\n",
    "    'vinai/PhoWhisper-tiny',\n",
    "    'vinai/PhoWhisper-base',\n",
    "    'vinai/PhoWhisper-small',\n",
    "    # 'vinai/PhoWhisper-medium',  # Uncomment if you have enough GPU memory (16GB+)\n",
    "    # 'vinai/PhoWhisper-large',   # Uncomment if you have enough GPU memory (24GB+)\n",
    "]\n",
    "\n",
    "# Dataset configuration - Select datasets to evaluate\n",
    "DATASETS_TO_TEST = [\n",
    "    'ViMD',\n",
    "    # 'BUD500',     # Uncomment to include (slower)\n",
    "    # 'LSVSC',      # Uncomment to include (slower)\n",
    "    # 'VLSP2020',   # Uncomment to include (slower)\n",
    "    # 'VietMed',    # Uncomment to include (slower)\n",
    "]\n",
    "\n",
    "# Sampling configuration\n",
    "MAX_SAMPLES_PER_SPLIT = 50  # Limit samples for faster evaluation. Set to None for full dataset\n",
    "\n",
    "# Output configuration\n",
    "TIMESTAMP = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "OUTPUT_CSV = f\"/content/phowhisper_results_{TIMESTAMP}.csv\"\n",
    "\n",
    "print(f'[CONFIG] Models to evaluate: {len(MODELS_TO_TEST)}')\n",
    "print(f'[CONFIG] Datasets to evaluate: {len(DATASETS_TO_TEST)}')\n",
    "print(f'[CONFIG] Max samples per split: {MAX_SAMPLES_PER_SPLIT or \"All\"}')\n",
    "print(f'[CONFIG] Output file: {OUTPUT_CSV}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Load Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`trust_remote_code` is not supported anymore.\n",
      "Please check that the Hugging Face dataset 'nguyendv02/ViMD_Dataset' isn't based on a loading script and remove `trust_remote_code`.\n",
      "If the dataset is based on a loading script, please ask the dataset author to remove it and convert it to a standard format like Parquet.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "Loading dataset: ViMD\n",
      "============================================================\n",
      "[INFO] Loading ViMD from HuggingFace Hub...\n",
      "  Loading train split...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Repo card metadata block was not found. Setting CardData to empty.\n",
      "Downloading data: 100%|██████████| 103/103 [00:00<00:00, 119.83files/s]\n",
      "Generating train split: 2190 examples [00:54, 39.96 examples/s]\n",
      "`trust_remote_code` is not supported anymore.\n",
      "Please check that the Hugging Face dataset 'nguyendv02/ViMD_Dataset' isn't based on a loading script and remove `trust_remote_code`.\n",
      "If the dataset is based on a loading script, please ask the dataset author to remove it and convert it to a standard format like Parquet.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  [WARNING] Failed to load train: An error occurred while generating the dataset\n",
      "  Loading test split...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Repo card metadata block was not found. Setting CardData to empty.\n",
      "Downloading data: 100%|██████████| 103/103 [00:00<00:00, 242.08files/s]\n",
      "Generating train split: 0 examples [00:00, ? examples/s]"
     ]
    }
   ],
   "source": [
    "# Load all datasets\n",
    "datasets_loaded = {}\n",
    "\n",
    "for dataset_name in DATASETS_TO_TEST:\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Loading dataset: {dataset_name}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    try:\n",
    "        splits = load_huggingface_dataset(dataset_name, max_samples=MAX_SAMPLES_PER_SPLIT)\n",
    "        datasets_loaded[dataset_name] = splits\n",
    "        \n",
    "        print(f\"\\n[OK] {dataset_name} loaded:\")\n",
    "        print(f\"  - Train: {len(splits['train'])} samples\")\n",
    "        print(f\"  - Val: {len(splits['val'])} samples\")\n",
    "        print(f\"  - Test: {len(splits['test'])} samples\")\n",
    "    except Exception as e:\n",
    "        print(f\"[ERROR] Failed to load {dataset_name}: {e}\")\n",
    "        datasets_loaded[dataset_name] = {'train': [], 'val': [], 'test': []}\n",
    "\n",
    "print(f\"\\n[OK] Successfully loaded {len(datasets_loaded)} datasets\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Run Evaluation\n",
    "\n",
    "This will evaluate all models on all datasets. **This may take 30-60 minutes depending on GPU and dataset size.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main evaluation loop\n",
    "results = []\n",
    "metrics_calculator = ASRMetrics()\n",
    "\n",
    "total_start_time = time.time()\n",
    "\n",
    "for model_id in MODELS_TO_TEST:\n",
    "    print(f\"\\n\\n{'='*70}\")\n",
    "    print(f\"EVALUATING MODEL: {model_id}\")\n",
    "    print(f\"{'='*70}\\n\")\n",
    "    \n",
    "    # Load model\n",
    "    try:\n",
    "        model = PhoWhisperModel(model_id)\n",
    "        model.load_model()\n",
    "    except Exception as e:\n",
    "        print(f\"[ERROR] Skipping {model_id}: {e}\")\n",
    "        continue\n",
    "    \n",
    "    # Evaluate on each dataset\n",
    "    for dataset_name, splits in datasets_loaded.items():\n",
    "        # Use test split for evaluation\n",
    "        test_samples = splits['test']\n",
    "        \n",
    "        if not test_samples:\n",
    "            print(f\"[WARNING] No test samples for {dataset_name}, skipping...\")\n",
    "            continue\n",
    "        \n",
    "        print(f\"\\n[INFO] Evaluating on {dataset_name} ({len(test_samples)} samples)...\")\n",
    "        \n",
    "        # Transcribe all samples\n",
    "        references = []\n",
    "        hypotheses = []\n",
    "        audio_durations = []\n",
    "        processing_times = []\n",
    "        \n",
    "        for sample in tqdm(test_samples, desc=f\"{dataset_name}\", leave=False):\n",
    "            try:\n",
    "                # Transcribe with timing\n",
    "                with RTFTimer() as timer:\n",
    "                    hypothesis = model.transcribe(sample.audio_path)\n",
    "                \n",
    "                references.append(sample.transcription)\n",
    "                hypotheses.append(hypothesis)\n",
    "                audio_durations.append(sample.duration)\n",
    "                processing_times.append(timer.elapsed_time)\n",
    "            except Exception as e:\n",
    "                print(f\"  [WARNING] Failed to process sample: {e}\")\n",
    "                continue\n",
    "        \n",
    "        # Calculate metrics\n",
    "        if references and hypotheses:\n",
    "            metrics = metrics_calculator.calculate_all_metrics(references, hypotheses)\n",
    "            \n",
    "            # Calculate RTF\n",
    "            total_audio_duration = sum(audio_durations)\n",
    "            total_processing_time = sum(processing_times)\n",
    "            rtf = total_processing_time / total_audio_duration if total_audio_duration > 0 else 0\n",
    "            \n",
    "            # Store results\n",
    "            result = {\n",
    "                'model': model_id,\n",
    "                'dataset': dataset_name,\n",
    "                'samples_processed': len(references),\n",
    "                'WER': metrics['wer'],\n",
    "                'CER': metrics['cer'],\n",
    "                'MER': metrics['mer'],\n",
    "                'WIL': metrics['wil'],\n",
    "                'WIP': metrics['wip'],\n",
    "                'SER': metrics['ser'],\n",
    "                'RTF': rtf,\n",
    "                'insertions': metrics['insertions'],\n",
    "                'deletions': metrics['deletions'],\n",
    "                'substitutions': metrics['substitutions'],\n",
    "                'total_audio_duration': total_audio_duration,\n",
    "                'total_processing_time': total_processing_time\n",
    "            }\n",
    "            results.append(result)\n",
    "            \n",
    "            print(f\"  [OK] WER: {metrics['wer']:.4f} | CER: {metrics['cer']:.4f} | RTF: {rtf:.4f}\")\n",
    "        else:\n",
    "            print(f\"  [WARNING] No valid results for {dataset_name}\")\n",
    "    \n",
    "    # Free memory\n",
    "    del model\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "total_evaluation_time = time.time() - total_start_time\n",
    "\n",
    "print(f\"\\n\\n{'='*70}\")\n",
    "print(f\"EVALUATION COMPLETE!\")\n",
    "print(f\"{'='*70}\")\n",
    "print(f\"Total time: {total_evaluation_time:.2f}s ({total_evaluation_time/60:.2f} minutes)\")\n",
    "print(f\"Total results: {len(results)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Display Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create results DataFrame\n",
    "results_df = pd.DataFrame(results)\n",
    "\n",
    "# Display all results\n",
    "print(\"\\n[INFO] Complete Evaluation Results:\")\n",
    "print(\"=\"*100)\n",
    "display_cols = ['model', 'dataset', 'WER', 'CER', 'MER', 'RTF', 'samples_processed']\n",
    "print(results_df[display_cols].to_string(index=False))\n",
    "\n",
    "# Summary statistics\n",
    "print(\"\\n\\n[CHART] Average Performance by Model:\")\n",
    "print(\"=\"*80)\n",
    "model_avg = results_df.groupby('model')[['WER', 'CER', 'MER', 'RTF']].mean()\n",
    "print(model_avg.to_string())\n",
    "\n",
    "# Find best model\n",
    "best_idx = results_df['WER'].idxmin()\n",
    "best = results_df.loc[best_idx]\n",
    "print(\"\\n\\n[TARGET] Best Model (Lowest WER):\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Model: {best['model']}\")\n",
    "print(f\"Dataset: {best['dataset']}\")\n",
    "print(f\"WER: {best['WER']:.4f}\")\n",
    "print(f\"CER: {best['CER']:.4f}\")\n",
    "print(f\"RTF: {best['RTF']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: Visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "sns.set_style(\"whitegrid\")\n",
    "\n",
    "# Plot 1: WER Comparison\n",
    "plt.figure(figsize=(14, 6))\n",
    "pivot_wer = results_df.pivot(index='dataset', columns='model', values='WER')\n",
    "pivot_wer.plot(kind='bar', ax=plt.gca())\n",
    "plt.title('Word Error Rate (WER) Comparison - PhoWhisper Models', fontsize=14, fontweight='bold')\n",
    "plt.xlabel('Dataset', fontsize=12)\n",
    "plt.ylabel('WER (Lower is Better)', fontsize=12)\n",
    "plt.legend(title='Model', bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Plot 2: CER Comparison\n",
    "plt.figure(figsize=(14, 6))\n",
    "pivot_cer = results_df.pivot(index='dataset', columns='model', values='CER')\n",
    "pivot_cer.plot(kind='bar', ax=plt.gca())\n",
    "plt.title('Character Error Rate (CER) Comparison - PhoWhisper Models', fontsize=14, fontweight='bold')\n",
    "plt.xlabel('Dataset', fontsize=12)\n",
    "plt.ylabel('CER (Lower is Better)', fontsize=12)\n",
    "plt.legend(title='Model', bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Plot 3: RTF Comparison\n",
    "plt.figure(figsize=(14, 6))\n",
    "pivot_rtf = results_df.pivot(index='dataset', columns='model', values='RTF')\n",
    "pivot_rtf.plot(kind='bar', ax=plt.gca())\n",
    "plt.axhline(y=1.0, color='r', linestyle='--', label='Real-time threshold')\n",
    "plt.title('Real-Time Factor (RTF) Comparison - PhoWhisper Models', fontsize=14, fontweight='bold')\n",
    "plt.xlabel('Dataset', fontsize=12)\n",
    "plt.ylabel('RTF (Lower is Better, <1.0 = Real-time)', fontsize=12)\n",
    "plt.legend(title='Model', bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Plot 4: Metrics Heatmap\n",
    "plt.figure(figsize=(16, 10))\n",
    "heatmap_data = results_df.set_index(['model', 'dataset'])[['WER', 'CER', 'MER', 'WIL', 'WIP', 'SER', 'RTF']]\n",
    "sns.heatmap(heatmap_data, annot=True, fmt='.4f', cmap='RdYlGn_r', cbar_kws={'label': 'Metric Value'})\n",
    "plt.title('All Metrics Heatmap - PhoWhisper Models', fontsize=14, fontweight='bold')\n",
    "plt.xlabel('Metric', fontsize=12)\n",
    "plt.ylabel('Model + Dataset', fontsize=12)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 8: Export Results to CSV\n",
    "\n",
    "**Download this CSV file** to use in the cross-model comparison notebook (05)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save to CSV\n",
    "results_df.to_csv(OUTPUT_CSV, index=False)\n",
    "print(f\"[OK] Results saved to: {OUTPUT_CSV}\")\n",
    "print(f\"\\n[INFO] Download this file to use in notebook 05 (cross-model comparison)\")\n",
    "print(f\"[INFO] File location: {OUTPUT_CSV}\")\n",
    "\n",
    "# Display download link (for Colab)\n",
    "try:\n",
    "    from google.colab import files\n",
    "    print(\"\\n[ACTION] Click below to download the results:\")\n",
    "    files.download(OUTPUT_CSV)\n",
    "except ImportError:\n",
    "    print(\"[INFO] Not running in Colab - file saved locally\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "### What This Notebook Did:\n",
    "1. Installed all required packages\n",
    "2. Loaded datasets from HuggingFace Hub\n",
    "3. Downloaded and evaluated PhoWhisper models\n",
    "4. Calculated comprehensive metrics (WER, CER, MER, WIL, WIP, SER, RTF)\n",
    "5. Generated visualization plots\n",
    "6. Exported results to CSV\n",
    "\n",
    "### Next Steps:\n",
    "1. Download the CSV file from `/content/phowhisper_results_*.csv`\n",
    "2. Run notebooks 02 (Whisper), 03 (Wav2Vec2), 04 (Wav2Vn) similarly\n",
    "3. Upload all CSV files to notebook 05 for cross-model comparison\n",
    "\n",
    "### Key Findings:\n",
    "- Best model: (See results above)\n",
    "- Average WER: (See summary statistics)\n",
    "- Real-time capable: (Models with RTF < 1.0)\n",
    "\n",
    "---\n",
    "\n",
    "**Vietnamese ASR Evaluation Framework v1.0 - Standalone Colab Edition**"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "lactech-stt",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
