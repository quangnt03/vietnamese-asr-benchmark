{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cross-Model Comparison and Analysis - Google Colab Standalone\n",
    "\n",
    "This notebook aggregates and compares results from **all model families**.\n",
    "\n",
    "## Prerequisites:\n",
    "Run notebooks 01-04 first and download their CSV result files:\n",
    "- phowhisper_results_*.csv\n",
    "- whisper_results_*.csv\n",
    "- wav2vec2_results_*.csv\n",
    "- wav2vn_results_*.csv (optional - mock results)\n",
    "\n",
    "## This Notebook Will:\n",
    "1. Upload and combine all CSV results\n",
    "2. Identify best models overall and per-dataset\n",
    "3. Perform statistical analysis (ANOVA)\n",
    "4. Create comprehensive visualizations\n",
    "5. Generate deployment recommendations\n",
    "\n",
    "**Runtime**: CPU sufficient (no model loading)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('[SETUP] Installing packages...')\n",
    "!pip install -q pandas numpy matplotlib seaborn scipy\n",
    "print('[OK] All packages installed!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Upload Result Files\n",
    "\n",
    "Upload the CSV files you downloaded from notebooks 01-04:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import files\n",
    "import pandas as pd\n",
    "import io\n",
    "\n",
    "print('[INFO] Please upload CSV result files from notebooks 01-04')\n",
    "print('[INFO] You can select multiple files at once')\n",
    "print()\n",
    "\n",
    "uploaded = files.upload()\n",
    "\n",
    "print(f'\\n[OK] Uploaded {len(uploaded)} file(s)')\n",
    "for filename in uploaded.keys():\n",
    "    print(f'  - {filename}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Load and Combine Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Load all uploaded CSV files\n",
    "all_results = []\n",
    "\n",
    "for filename in uploaded.keys():\n",
    "    try:\n",
    "        df = pd.read_csv(io.BytesIO(uploaded[filename]))\n",
    "        \n",
    "        # Add model_family column based on filename or model name\n",
    "        if 'phowhisper' in filename.lower() or 'phowhisper' in str(df['model'].iloc[0]).lower():\n",
    "            df['model_family'] = 'PhoWhisper'\n",
    "        elif 'whisper' in filename.lower() and 'pho' not in filename.lower():\n",
    "            df['model_family'] = 'Whisper'\n",
    "        elif 'wav2vec2' in filename.lower():\n",
    "            df['model_family'] = 'Wav2Vec2'\n",
    "        elif 'wav2vn' in filename.lower():\n",
    "            df['model_family'] = 'Wav2Vn'\n",
    "        else:\n",
    "            df['model_family'] = 'Unknown'\n",
    "        \n",
    "        all_results.append(df)\n",
    "        print(f'[OK] Loaded {filename}: {len(df)} rows, family={df[\"model_family\"].iloc[0]}')\n",
    "    except Exception as e:\n",
    "        print(f'[ERROR] Failed to load {filename}: {e}')\n",
    "\n",
    "# Combine all results\n",
    "if all_results:\n",
    "    combined_df = pd.concat(all_results, ignore_index=True)\n",
    "    print(f'\\n[OK] Combined {len(all_results)} files into {len(combined_df)} total results')\n",
    "    print(f'[INFO] Model families: {combined_df[\"model_family\"].unique().tolist()}')\n",
    "    print(f'[INFO] Datasets: {combined_df[\"dataset\"].unique().tolist()}')\n",
    "    print(f'[INFO] Models: {len(combined_df[\"model\"].unique())}')\n",
    "else:\n",
    "    print('[ERROR] No results loaded!')\n",
    "    combined_df = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Display Complete Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not combined_df.empty:\n",
    "    print('[INFO] Complete Evaluation Results')\n",
    "    print('='*100)\n",
    "    display_cols = ['model_family', 'model', 'dataset', 'WER', 'CER', 'MER', 'RTF', 'samples_processed']\n",
    "    print(combined_df[display_cols].to_string(index=False))\n",
    "    \n",
    "    # Summary statistics\n",
    "    print('\\n\\n[CHART] Average Performance by Model Family:')\n",
    "    print('='*80)\n",
    "    family_avg = combined_df.groupby('model_family')[['WER', 'CER', 'MER', 'RTF']].mean()\n",
    "    print(family_avg.to_string())\n",
    "    \n",
    "    print('\\n\\n[CHART] Average Performance by Model:')\n",
    "    print('='*80)\n",
    "    model_avg = combined_df.groupby('model')[['WER', 'CER', 'MER', 'RTF']].mean().sort_values('WER')\n",
    "    print(model_avg.to_string())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Find Best Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not combined_df.empty:\n",
    "    print('[TARGET] BEST MODELS OVERALL')\n",
    "    print('='*80)\n",
    "    \n",
    "    # Best by WER\n",
    "    best_wer_idx = combined_df['WER'].idxmin()\n",
    "    best_wer = combined_df.loc[best_wer_idx]\n",
    "    print(f'\\n[1] Best WER:')\n",
    "    print(f'    Model: {best_wer[\"model\"]}')\n",
    "    print(f'    Dataset: {best_wer[\"dataset\"]}')\n",
    "    print(f'    WER: {best_wer[\"WER\"]:.4f}')\n",
    "    print(f'    CER: {best_wer[\"CER\"]:.4f}')\n",
    "    print(f'    RTF: {best_wer[\"RTF\"]:.4f}')\n",
    "    \n",
    "    # Best by RTF (fastest)\n",
    "    best_rtf_idx = combined_df['RTF'].idxmin()\n",
    "    best_rtf = combined_df.loc[best_rtf_idx]\n",
    "    print(f'\\n[2] Fastest (Best RTF):')\n",
    "    print(f'    Model: {best_rtf[\"model\"]}')\n",
    "    print(f'    Dataset: {best_rtf[\"dataset\"]}')\n",
    "    print(f'    WER: {best_rtf[\"WER\"]:.4f}')\n",
    "    print(f'    RTF: {best_rtf[\"RTF\"]:.4f}')\n",
    "    \n",
    "    # Best balanced (WER * RTF)\n",
    "    combined_df['balance_score'] = combined_df['WER'] * combined_df['RTF']\n",
    "    best_balance_idx = combined_df['balance_score'].idxmin()\n",
    "    best_balance = combined_df.loc[best_balance_idx]\n",
    "    print(f'\\n[3] Best Balanced (Speed + Accuracy):')\n",
    "    print(f'    Model: {best_balance[\"model\"]}')\n",
    "    print(f'    Dataset: {best_balance[\"dataset\"]}')\n",
    "    print(f'    WER: {best_balance[\"WER\"]:.4f}')\n",
    "    print(f'    RTF: {best_balance[\"RTF\"]:.4f}')\n",
    "    \n",
    "    # Best per dataset\n",
    "    print('\\n\\n[LIST] Best Model per Dataset (by WER):')\n",
    "    print('='*80)\n",
    "    for dataset in combined_df['dataset'].unique():\n",
    "        dataset_df = combined_df[combined_df['dataset'] == dataset]\n",
    "        best_idx = dataset_df['WER'].idxmin()\n",
    "        best = dataset_df.loc[best_idx]\n",
    "        print(f'\\n{dataset}:')\n",
    "        print(f'  Model: {best[\"model\"]}')\n",
    "        print(f'  WER: {best[\"WER\"]:.4f} | CER: {best[\"CER\"]:.4f} | RTF: {best[\"RTF\"]:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Statistical Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import stats\n",
    "\n",
    "if not combined_df.empty and len(combined_df['model_family'].unique()) >= 2:\n",
    "    print('[INFO] Statistical Significance Testing (ANOVA)')\n",
    "    print('='*80)\n",
    "    \n",
    "    # ANOVA for WER across model families\n",
    "    families = [group['WER'].values for name, group in combined_df.groupby('model_family')]\n",
    "    f_stat, p_value = stats.f_oneway(*families)\n",
    "    \n",
    "    print(f'\\nWER across Model Families:')\n",
    "    print(f'  F-statistic: {f_stat:.4f}')\n",
    "    print(f'  P-value: {p_value:.6f}')\n",
    "    \n",
    "    if p_value < 0.05:\n",
    "        print(f'  Result: Statistically SIGNIFICANT difference (p < 0.05)')\n",
    "        print(f'  Interpretation: Model families perform differently')\n",
    "    else:\n",
    "        print(f'  Result: No significant difference (p >= 0.05)')\n",
    "        print(f'  Interpretation: Model families perform similarly')\n",
    "    \n",
    "    # Correlation analysis\n",
    "    print(f'\\n\\n[CHART] Metric Correlations:')\n",
    "    metric_cols = ['WER', 'CER', 'MER', 'WIL', 'WIP', 'SER', 'RTF']\n",
    "    available_metrics = [m for m in metric_cols if m in combined_df.columns]\n",
    "    correlation = combined_df[available_metrics].corr()\n",
    "    print(correlation.to_string())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: Comprehensive Visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "sns.set_style(\"whitegrid\")\n",
    "\n",
    "if not combined_df.empty:\n",
    "    # Plot 1: WER Comparison\n",
    "    fig, ax = plt.subplots(figsize=(16, 8))\n",
    "    pivot_wer = combined_df.pivot_table(index='dataset', columns='model', values='WER', aggfunc='mean')\n",
    "    pivot_wer.plot(kind='bar', ax=ax, width=0.8)\n",
    "    ax.set_title('Word Error Rate (WER) - All Models Comparison', fontsize=16, fontweight='bold')\n",
    "    ax.set_xlabel('Dataset', fontsize=13)\n",
    "    ax.set_ylabel('WER (Lower is Better)', fontsize=13)\n",
    "    ax.legend(title='Model', bbox_to_anchor=(1.02, 1), loc='upper left', fontsize=9)\n",
    "    ax.set_xticklabels(ax.get_xticklabels(), rotation=45, ha='right')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Plot 2: Model Family Boxplots\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "    \n",
    "    sns.boxplot(data=combined_df, x='model_family', y='WER', ax=axes[0, 0])\n",
    "    axes[0, 0].set_title('WER Distribution by Model Family', fontweight='bold')\n",
    "    axes[0, 0].set_ylabel('WER')\n",
    "    \n",
    "    sns.boxplot(data=combined_df, x='model_family', y='CER', ax=axes[0, 1])\n",
    "    axes[0, 1].set_title('CER Distribution by Model Family', fontweight='bold')\n",
    "    axes[0, 1].set_ylabel('CER')\n",
    "    \n",
    "    sns.boxplot(data=combined_df, x='model_family', y='RTF', ax=axes[1, 0])\n",
    "    axes[1, 0].set_title('RTF Distribution by Model Family', fontweight='bold')\n",
    "    axes[1, 0].set_ylabel('RTF')\n",
    "    axes[1, 0].axhline(y=1.0, color='r', linestyle='--', linewidth=1, label='Real-time')\n",
    "    axes[1, 0].legend()\n",
    "    \n",
    "    sns.boxplot(data=combined_df, x='model_family', y='WIP', ax=axes[1, 1])\n",
    "    axes[1, 1].set_title('WIP Distribution by Model Family', fontweight='bold')\n",
    "    axes[1, 1].set_ylabel('WIP (Higher is Better)')\n",
    "    \n",
    "    for ax in axes.flat:\n",
    "        ax.set_xlabel('Model Family')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Plot 3: Speed vs Accuracy Scatter\n",
    "    fig, ax = plt.subplots(figsize=(14, 8))\n",
    "    \n",
    "    for family in combined_df['model_family'].unique():\n",
    "        family_df = combined_df[combined_df['model_family'] == family]\n",
    "        ax.scatter(family_df['RTF'], family_df['WER'], label=family, s=100, alpha=0.6)\n",
    "    \n",
    "    ax.axvline(x=1.0, color='red', linestyle='--', linewidth=2, alpha=0.5, label='Real-time threshold')\n",
    "    ax.set_xlabel('Real-Time Factor (RTF) - Lower is Faster', fontsize=13)\n",
    "    ax.set_ylabel('Word Error Rate (WER) - Lower is Better', fontsize=13)\n",
    "    ax.set_title('Speed vs Accuracy Trade-off: RTF vs WER', fontsize=16, fontweight='bold')\n",
    "    ax.legend(loc='best')\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    ax.text(0.05, 0.95, 'IDEAL\\n(Fast + Accurate)', transform=ax.transAxes, \n",
    "           fontsize=12, verticalalignment='top', \n",
    "           bbox=dict(boxstyle='round', facecolor='lightgreen', alpha=0.3))\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Plot 4: Comprehensive Heatmap\n",
    "    fig, ax = plt.subplots(figsize=(18, max(12, len(combined_df) * 0.4)))\n",
    "    heatmap_data = combined_df.set_index(['model', 'dataset'])[['WER', 'CER', 'MER', 'WIL', 'WIP', 'SER', 'RTF']]\n",
    "    sns.heatmap(heatmap_data, annot=True, fmt='.3f', cmap='RdYlGn_r', \n",
    "                cbar_kws={'label': 'Metric Value'}, ax=ax, linewidths=0.5)\n",
    "    ax.set_title('Comprehensive Metrics Heatmap - All Models & Datasets', fontsize=16, fontweight='bold')\n",
    "    ax.set_xlabel('Metric', fontsize=13)\n",
    "    ax.set_ylabel('Model + Dataset', fontsize=13)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print('[OK] All visualizations generated!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 8: Production Deployment Recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not combined_df.empty:\n",
    "    print('[TARGET] PRODUCTION DEPLOYMENT RECOMMENDATIONS')\n",
    "    print('='*80)\n",
    "    \n",
    "    # Best for accuracy\n",
    "    model_wer_avg = combined_df.groupby('model')['WER'].mean().sort_values()\n",
    "    best_accuracy = model_wer_avg.index[0]\n",
    "    best_accuracy_wer = model_wer_avg.iloc[0]\n",
    "    \n",
    "    # Best for speed\n",
    "    model_rtf_avg = combined_df.groupby('model')['RTF'].mean().sort_values()\n",
    "    best_speed = model_rtf_avg.index[0]\n",
    "    best_speed_rtf = model_rtf_avg.iloc[0]\n",
    "    \n",
    "    # Best balanced\n",
    "    model_scores = combined_df.groupby('model').agg({'WER': 'mean', 'RTF': 'mean'})\n",
    "    model_scores['balance'] = model_scores['WER'] * model_scores['RTF']\n",
    "    best_balanced = model_scores['balance'].idxmin()\n",
    "    balanced_wer = model_scores.loc[best_balanced, 'WER']\n",
    "    balanced_rtf = model_scores.loc[best_balanced, 'RTF']\n",
    "    \n",
    "    print(f'\\n1. BEST FOR ACCURACY (Lowest WER):')\n",
    "    print(f'   Recommendation: {best_accuracy}')\n",
    "    print(f'   Average WER: {best_accuracy_wer:.4f}')\n",
    "    print(f'   Use case: Offline transcription, high accuracy requirements')\n",
    "    \n",
    "    print(f'\\n2. BEST FOR SPEED (Lowest RTF):')\n",
    "    print(f'   Recommendation: {best_speed}')\n",
    "    print(f'   Average RTF: {best_speed_rtf:.4f}')\n",
    "    print(f'   Use case: Real-time transcription, latency-sensitive applications')\n",
    "    \n",
    "    print(f'\\n3. BEST BALANCED (Speed + Accuracy):')\n",
    "    print(f'   Recommendation: {best_balanced}')\n",
    "    print(f'   Average WER: {balanced_wer:.4f}')\n",
    "    print(f'   Average RTF: {balanced_rtf:.4f}')\n",
    "    print(f'   Use case: General-purpose transcription')\n",
    "    \n",
    "    print(f'\\n4. DATASET-SPECIFIC RECOMMENDATIONS:')\n",
    "    for dataset in combined_df['dataset'].unique():\n",
    "        dataset_df = combined_df[combined_df['dataset'] == dataset]\n",
    "        best_model = dataset_df.loc[dataset_df['WER'].idxmin(), 'model']\n",
    "        best_wer = dataset_df['WER'].min()\n",
    "        print(f'   {dataset}: {best_model} (WER: {best_wer:.4f})')\n",
    "    \n",
    "    print(f'\\n' + '='*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 9: Save Combined Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "if not combined_df.empty:\n",
    "    # Save combined results\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    output_file = f'/content/combined_results_{timestamp}.csv'\n",
    "    combined_df.to_csv(output_file, index=False)\n",
    "    \n",
    "    print(f'[OK] Combined results saved to: {output_file}')\n",
    "    print(f'[INFO] Total rows: {len(combined_df)}')\n",
    "    print(f'[INFO] Columns: {list(combined_df.columns)}')\n",
    "    \n",
    "    # Download\n",
    "    try:\n",
    "        files.download(output_file)\n",
    "        print(f'\\n[OK] File ready for download!')\n",
    "    except:\n",
    "        print(f'\\n[INFO] File saved at: {output_file}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "### What This Notebook Did:\n",
    "1. Uploaded and combined CSV results from notebooks 01-04\n",
    "2. Identified best models overall and per-dataset\n",
    "3. Performed statistical significance testing (ANOVA)\n",
    "4. Created comprehensive comparison visualizations\n",
    "5. Generated production deployment recommendations\n",
    "6. Saved combined results to CSV\n",
    "\n",
    "### Key Insights:\n",
    "- **Best Overall**: (See recommendations above)\n",
    "- **Fastest**: (See speed analysis)\n",
    "- **Best Balanced**: (See balanced recommendations)\n",
    "- **Statistical Significance**: (See ANOVA results)\n",
    "\n",
    "### Files Generated:\n",
    "- combined_results_*.csv - All results in one file\n",
    "\n",
    "---\n",
    "\n",
    "**Vietnamese ASR Evaluation Framework v1.0 - Cross-Model Comparison Edition**"
   ]
  }
 ],
 "metadata": {
  "colab": {},
  "kernelspec": {"display_name": "Python 3", "name": "python3"}
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
