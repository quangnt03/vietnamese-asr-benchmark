{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Wav2Vn Model Evaluation - Google Colab Standalone\n",
    "\n",
    "**IMPORTANT NOTE**: Wav2Vn model is not publicly available. This notebook uses **mock transcription** for demonstration purposes.\n",
    "\n",
    "## Purpose:\n",
    "- Demonstrates the evaluation pipeline\n",
    "- Creates placeholder results for cross-model comparison\n",
    "- Can be updated when Wav2Vn becomes available\n",
    "\n",
    "## Features:\n",
    "- Standalone execution\n",
    "- Mock transcription with consistent results\n",
    "- Full dataset loading (respects existing splits)\n",
    "- Exports CSV for notebook 05\n",
    "\n",
    "**Runtime**: CPU is sufficient (no real model inference)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('[SETUP] Installing packages...')\n",
    "!pip install -q datasets soundfile jiwer torch torchcodec torchaudio librosa soundfile jiwer datasets accelerate pandas matplotlib seaborn scipy tqdm\n",
    "print('[OK] Packages installed!')\n",
    "\n",
    "print('\\n[WARNING] This notebook uses MOCK TRANSCRIPTION')\n",
    "print('[WARNING] Wav2Vn model is not publicly available')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Embedded code (same as other notebooks)\n",
    "import time, warnings, hashlib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from dataclasses import dataclass\n",
    "from typing import List, Dict\n",
    "from tqdm.auto import tqdm\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from jiwer import wer, cer, mer, wil, wip, process_words\n",
    "from datasets import load_dataset, Audio\n",
    "import soundfile as sf\n",
    "import tempfile\n",
    "\n",
    "class ASRMetrics:\n",
    "    @staticmethod\n",
    "    def calculate_all_metrics(references: List[str], hypotheses: List[str]) -> Dict:\n",
    "        ref_text = ' '.join(references)\n",
    "        hyp_text = ' '.join(hypotheses)\n",
    "        output = process_words(ref_text, hyp_text)\n",
    "        return {\n",
    "            'wer': wer(ref_text, hyp_text), 'cer': cer(ref_text, hyp_text),\n",
    "            'mer': mer(ref_text, hyp_text), 'wil': wil(ref_text, hyp_text),\n",
    "            'wip': wip(ref_text, hyp_text),\n",
    "            'ser': sum(1 for r, h in zip(references, hypotheses) if r != h) / len(references),\n",
    "            'insertions': output.insertions, 'deletions': output.deletions,\n",
    "            'substitutions': output.substitutions\n",
    "        }\n",
    "\n",
    "class RTFTimer:\n",
    "    def __init__(self):\n",
    "        self.elapsed_time = None\n",
    "    def __enter__(self):\n",
    "        self.start = time.time()\n",
    "        return self\n",
    "    def __exit__(self, *args):\n",
    "        self.elapsed_time = time.time() - self.start\n",
    "\n",
    "@dataclass\n",
    "class AudioSample:\n",
    "    audio_path: str\n",
    "    transcription: str\n",
    "    duration: float = 0.0\n",
    "    sample_rate: int = 16000\n",
    "    dataset: str = ''\n",
    "    split: str = ''\n",
    "\n",
    "def load_huggingface_dataset(dataset_name: str, max_samples: int = None) -> Dict:\n",
    "    configs = {\n",
    "        'ViMD': {'id': 'nguyendv02/ViMD_Dataset', 'splits': ['train', 'test', 'valid'], \n",
    "                 'audio_col': 'audio', 'text_col': 'text'},\n",
    "        'BUD500': {'id': 'linhtran92/viet_bud500', 'splits': ['train', 'validation', 'test'], \n",
    "                   'audio_col': 'audio', 'text_col': 'transcription'},\n",
    "        'LSVSC': {'id': 'doof-ferb/LSVSC', 'splits': ['train', 'validation', 'test'], \n",
    "                  'audio_col': 'audio', 'text_col': 'transcription'},\n",
    "        'VLSP2020': {'id': 'doof-ferb/vlsp2020_vinai_100h', 'splits': ['train'], \n",
    "                     'audio_col': 'audio', 'text_col': 'transcription'},\n",
    "        'VietMed': {'id': 'leduckhai/VietMed', 'splits': ['train', 'test', 'dev'], \n",
    "                    'audio_col': 'audio', 'text_col': 'text'}\n",
    "    }\n",
    "    config = configs[dataset_name]\n",
    "    samples_by_split = {'train': [], 'val': [], 'test': []}\n",
    "    temp_dir = Path(tempfile.gettempdir()) / 'asr_audio' / dataset_name\n",
    "    temp_dir.mkdir(parents=True, exist_ok=True)\n",
    "    all_samples = []\n",
    "    \n",
    "    for split in config['splits']:\n",
    "        try:\n",
    "            dataset = load_dataset(config['id'], split=split, trust_remote_code=True)\n",
    "            if config['audio_col'] in dataset.column_names:\n",
    "                dataset = dataset.cast_column(config['audio_col'], Audio(sampling_rate=16000))\n",
    "            if max_samples and len(dataset) > max_samples:\n",
    "                dataset = dataset.select(range(max_samples))\n",
    "            \n",
    "            samples = []\n",
    "            for idx, item in enumerate(tqdm(dataset, desc=f\"{split}\", leave=False)):\n",
    "                try:\n",
    "                    audio_data = item[config['audio_col']]\n",
    "                    audio_path = str(temp_dir / f\"{split}_{idx}.wav\")\n",
    "                    sf.write(audio_path, audio_data['array'], audio_data['sampling_rate'])\n",
    "                    sample = AudioSample(\n",
    "                        audio_path=audio_path,\n",
    "                        transcription=str(item[config['text_col']]).strip().lower(),\n",
    "                        duration=len(audio_data['array']) / audio_data['sampling_rate'],\n",
    "                        dataset=dataset_name, split=split\n",
    "                    )\n",
    "                    samples.append(sample)\n",
    "                except:\n",
    "                    continue\n",
    "            \n",
    "            if split in ['train', 'training']:\n",
    "                samples_by_split['train'].extend(samples)\n",
    "            elif split in ['val', 'validation', 'dev', 'valid']:\n",
    "                samples_by_split['val'].extend(samples)\n",
    "            elif split in ['test', 'testing']:\n",
    "                samples_by_split['test'].extend(samples)\n",
    "            all_samples.extend(samples)\n",
    "        except:\n",
    "            pass\n",
    "    \n",
    "    if dataset_name == 'VLSP2020' and all_samples:\n",
    "        np.random.seed(42)\n",
    "        indices = np.random.permutation(len(all_samples))\n",
    "        train_end, val_end = int(len(all_samples)*0.7), int(len(all_samples)*0.85)\n",
    "        samples_by_split['train'] = [all_samples[i] for i in indices[:train_end]]\n",
    "        samples_by_split['val'] = [all_samples[i] for i in indices[train_end:val_end]]\n",
    "        samples_by_split['test'] = [all_samples[i] for i in indices[val_end:]]\n",
    "    \n",
    "    return samples_by_split\n",
    "\n",
    "class Wav2VnModel:\n",
    "    \"\"\"Mock model for Wav2Vn (not publicly available).\"\"\"\n",
    "    def __init__(self):\n",
    "        self.mock_texts = [\n",
    "            \"xin chào tôi là người việt nam\",\n",
    "            \"hôm nay thời tiết đẹp\",\n",
    "            \"tôi yêu tiếng việt\",\n",
    "            \"chúng tôi đang học máy học\",\n",
    "            \"đây là bài kiểm tra\"\n",
    "        ]\n",
    "    \n",
    "    def load_model(self):\n",
    "        print(\"[WARNING] Using mock transcription (Wav2Vn not available)\")\n",
    "    \n",
    "    def transcribe(self, audio_path: str) -> str:\n",
    "        hash_val = int(hashlib.md5(audio_path.encode()).hexdigest(), 16)\n",
    "        return self.mock_texts[hash_val % len(self.mock_texts)]\n",
    "\n",
    "print('[OK] Helper functions loaded (with mock Wav2Vn model)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "DATASETS_TO_TEST = ['ViMD']  # Add more as needed\n",
    "MAX_SAMPLES_PER_SPLIT = None  # None = full dataset\n",
    "TIMESTAMP = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "OUTPUT_CSV = f\"/content/wav2vn_results_{TIMESTAMP}.csv\"\n",
    "\n",
    "print(f'[CONFIG] Using MOCK transcription for Wav2Vn')\n",
    "print(f'[CONFIG] Datasets: {DATASETS_TO_TEST}')\n",
    "print(f'[CONFIG] Output: {OUTPUT_CSV}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load datasets\n",
    "datasets_loaded = {}\n",
    "for dataset_name in DATASETS_TO_TEST:\n",
    "    try:\n",
    "        splits = load_huggingface_dataset(dataset_name, MAX_SAMPLES_PER_SPLIT)\n",
    "        datasets_loaded[dataset_name] = splits\n",
    "        print(f\"[OK] {dataset_name}: train={len(splits['train'])}, \"\n",
    "              f\"val={len(splits['val'])}, test={len(splits['test'])}\")\n",
    "    except Exception as e:\n",
    "        print(f\"[ERROR] {dataset_name}: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run mock evaluation\n",
    "results = []\n",
    "model = Wav2VnModel()\n",
    "model.load_model()\n",
    "metrics_calc = ASRMetrics()\n",
    "\n",
    "for dataset_name, splits in datasets_loaded.items():\n",
    "    test_samples = splits['test']\n",
    "    if not test_samples:\n",
    "        continue\n",
    "    \n",
    "    print(f\"\\n[INFO] Evaluating on {dataset_name} ({len(test_samples)} samples)\")\n",
    "    refs, hyps, durations, times = [], [], [], []\n",
    "    \n",
    "    for sample in tqdm(test_samples, desc=dataset_name):\n",
    "        with RTFTimer() as timer:\n",
    "            hyp = model.transcribe(sample.audio_path)\n",
    "        refs.append(sample.transcription)\n",
    "        hyps.append(hyp)\n",
    "        durations.append(sample.duration)\n",
    "        times.append(timer.elapsed_time)\n",
    "    \n",
    "    if refs:\n",
    "        metrics = metrics_calc.calculate_all_metrics(refs, hyps)\n",
    "        rtf = sum(times) / sum(durations) if sum(durations) > 0 else 0\n",
    "        \n",
    "        results.append({\n",
    "            'model': 'wav2vn-mock',\n",
    "            'dataset': dataset_name,\n",
    "            'samples_processed': len(refs),\n",
    "            'WER': metrics['wer'], 'CER': metrics['cer'], 'MER': metrics['mer'],\n",
    "            'WIL': metrics['wil'], 'WIP': metrics['wip'], 'SER': metrics['ser'],\n",
    "            'RTF': rtf,\n",
    "            'insertions': metrics['insertions'],\n",
    "            'deletions': metrics['deletions'],\n",
    "            'substitutions': metrics['substitutions'],\n",
    "            'total_audio_duration': sum(durations),\n",
    "            'total_processing_time': sum(times)\n",
    "        })\n",
    "        print(f\"  [OK] WER: {metrics['wer']:.4f} (MOCK RESULTS)\")\n",
    "\n",
    "print(\"\\n[WARNING] Results are from MOCK transcription\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save results\n",
    "results_df = pd.DataFrame(results)\n",
    "print(results_df[['model', 'dataset', 'WER', 'CER', 'RTF']].to_string(index=False))\n",
    "\n",
    "results_df.to_csv(OUTPUT_CSV, index=False)\n",
    "print(f\"\\n[OK] Results saved: {OUTPUT_CSV}\")\n",
    "print(\"[WARNING] These are MOCK results for demonstration only\")\n",
    "\n",
    "try:\n",
    "    from google.colab import files\n",
    "    files.download(OUTPUT_CSV)\n",
    "except:\n",
    "    print(\"[INFO] File saved locally\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "**IMPORTANT**: This notebook uses mock transcription because Wav2Vn is not publicly available.\n",
    "\n",
    "### To use real Wav2Vn:\n",
    "1. Obtain the model from the authors\n",
    "2. Update `Wav2VnModel` class with real model loading\n",
    "3. Re-run this notebook\n",
    "\n",
    "### Current status:\n",
    "- CSV file exported for cross-model comparison\n",
    "- Results marked as 'wav2vn-mock'\n",
    "- Can be filtered out or replaced later\n",
    "\n",
    "---\n",
    "**Vietnamese ASR Evaluation Framework - Wav2Vn Mock Edition**"
   ]
  }
 ],
 "metadata": {
  "colab": {},
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
