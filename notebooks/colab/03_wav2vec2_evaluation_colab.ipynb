{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Wav2Vec2-XLSR Model Evaluation - Google Colab Standalone\n",
    "\n",
    "This notebook evaluates **Wav2Vec2-XLSR Vietnamese models** on Vietnamese datasets.\n",
    "\n",
    "## Models Evaluated:\n",
    "- anuragshas/wav2vec2-large-xlsr-53-vietnamese\n",
    "- nguyenvulebinh/wav2vec2-base-vietnamese-250h\n",
    "\n",
    "## Features:\n",
    "- Complete standalone execution (no external files needed)\n",
    "- Downloads datasets from HuggingFace automatically\n",
    "- Uses full datasets (respects existing train/val/test splits)\n",
    "- Calculates comprehensive metrics (WER, CER, MER, WIL, WIP, SER, RTF)\n",
    "- Generates visualizations\n",
    "- Exports results as CSV for cross-model comparison\n",
    "\n",
    "**Runtime**: GPU recommended (T4 or better)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('[SETUP] Installing required packages...')\n",
    "!pip install -q transformers==4.57.1 torch torchcodec torchaudio librosa soundfile jiwer datasets accelerate pandas matplotlib seaborn scipy tqdm\n",
    "print('[OK] All packages installed successfully!')\n",
    "\n",
    "# Check GPU availability\n",
    "import torch\n",
    "print(f'\\n[INFO] CUDA available: {torch.cuda.is_available()}')\n",
    "if torch.cuda.is_available():\n",
    "    print(f'[INFO] GPU: {torch.cuda.get_device_name(0)}')\n",
    "else:\n",
    "    print('[WARNING] Running on CPU - evaluation will be slower')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Embedded Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Embedded Vietnamese ASR Evaluation Code\n",
    "\n",
    "import time\n",
    "import warnings\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from dataclasses import dataclass\n",
    "from typing import List, Dict, Optional\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# METRICS MODULE\n",
    "from jiwer import wer, cer, mer, wil, wip, process_words\n",
    "\n",
    "class ASRMetrics:\n",
    "    @staticmethod\n",
    "    def calculate_all_metrics(references: List[str], hypotheses: List[str]) -> Dict:\n",
    "        ref_text = ' '.join(references)\n",
    "        hyp_text = ' '.join(hypotheses)\n",
    "        output = process_words(ref_text, hyp_text)\n",
    "        return {\n",
    "            'wer': wer(ref_text, hyp_text),\n",
    "            'cer': cer(ref_text, hyp_text),\n",
    "            'mer': mer(ref_text, hyp_text),\n",
    "            'wil': wil(ref_text, hyp_text),\n",
    "            'wip': wip(ref_text, hyp_text),\n",
    "            'ser': sum(1 for r, h in zip(references, hypotheses) if r != h) / len(references),\n",
    "            'insertions': output.insertions,\n",
    "            'deletions': output.deletions,\n",
    "            'substitutions': output.substitutions\n",
    "        }\n",
    "\n",
    "class RTFTimer:\n",
    "    def __init__(self):\n",
    "        self.start_time = None\n",
    "        self.elapsed_time = None\n",
    "    def __enter__(self):\n",
    "        self.start_time = time.time()\n",
    "        return self\n",
    "    def __exit__(self, exc_type, exc_val, exc_tb):\n",
    "        self.elapsed_time = time.time() - self.start_time\n",
    "\n",
    "# DATASET LOADER MODULE\n",
    "from datasets import load_dataset, Audio\n",
    "import soundfile as sf\n",
    "import tempfile\n",
    "\n",
    "@dataclass\n",
    "class AudioSample:\n",
    "    audio_path: str\n",
    "    transcription: str\n",
    "    duration: float = 0.0\n",
    "    sample_rate: int = 16000\n",
    "    dataset: str = ''\n",
    "    split: str = ''\n",
    "\n",
    "def load_huggingface_dataset(dataset_name: str, max_samples: int = None) -> Dict[str, List[AudioSample]]:\n",
    "    \"\"\"Load dataset with proper split handling.\"\"\"\n",
    "    configs = {\n",
    "        'ViMD': {'id': 'nguyendv02/ViMD_Dataset', 'splits': ['train', 'test', 'valid'], \n",
    "                 'audio_col': 'audio', 'text_col': 'text'},\n",
    "        'BUD500': {'id': 'linhtran92/viet_bud500', 'splits': ['train', 'validation', 'test'], \n",
    "                   'audio_col': 'audio', 'text_col': 'transcription'},\n",
    "        'LSVSC': {'id': 'doof-ferb/LSVSC', 'splits': ['train', 'validation', 'test'], \n",
    "                  'audio_col': 'audio', 'text_col': 'transcription'},\n",
    "        'VLSP2020': {'id': 'doof-ferb/vlsp2020_vinai_100h', 'splits': ['train'], \n",
    "                     'audio_col': 'audio', 'text_col': 'transcription'},\n",
    "        'VietMed': {'id': 'leduckhai/VietMed', 'splits': ['train', 'test', 'dev'], \n",
    "                    'audio_col': 'audio', 'text_col': 'text'}\n",
    "    }\n",
    "    \n",
    "    config = configs[dataset_name]\n",
    "    print(f\"[INFO] Loading {dataset_name} from HuggingFace Hub...\")\n",
    "    \n",
    "    samples_by_split = {'train': [], 'val': [], 'test': []}\n",
    "    temp_dir = Path(tempfile.gettempdir()) / 'asr_audio' / dataset_name\n",
    "    temp_dir.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    all_samples = []\n",
    "    for split in config['splits']:\n",
    "        try:\n",
    "            print(f\"  Loading {split} split...\")\n",
    "            dataset = load_dataset(config['id'], split=split, trust_remote_code=True)\n",
    "            \n",
    "            if config['audio_col'] in dataset.column_names:\n",
    "                dataset = dataset.cast_column(config['audio_col'], Audio(sampling_rate=16000))\n",
    "            \n",
    "            # Limit only if specified (for quick testing)\n",
    "            if max_samples and len(dataset) > max_samples:\n",
    "                dataset = dataset.select(range(max_samples))\n",
    "            \n",
    "            samples = []\n",
    "            for idx, item in enumerate(tqdm(dataset, desc=f\"{split}\", leave=False)):\n",
    "                try:\n",
    "                    audio_data = item[config['audio_col']]\n",
    "                    audio_path = str(temp_dir / f\"{split}_{idx}.wav\")\n",
    "                    sf.write(audio_path, audio_data['array'], audio_data['sampling_rate'])\n",
    "                    duration = len(audio_data['array']) / audio_data['sampling_rate']\n",
    "                    transcription = str(item[config['text_col']]).strip().lower()\n",
    "                    \n",
    "                    sample = AudioSample(\n",
    "                        audio_path=audio_path,\n",
    "                        transcription=transcription,\n",
    "                        duration=duration,\n",
    "                        sample_rate=audio_data['sampling_rate'],\n",
    "                        dataset=dataset_name,\n",
    "                        split=split\n",
    "                    )\n",
    "                    samples.append(sample)\n",
    "                except Exception as e:\n",
    "                    continue\n",
    "            \n",
    "            # Map to standard splits\n",
    "            if split in ['train', 'training']:\n",
    "                samples_by_split['train'].extend(samples)\n",
    "            elif split in ['val', 'validation', 'dev', 'valid']:\n",
    "                samples_by_split['val'].extend(samples)\n",
    "            elif split in ['test', 'testing']:\n",
    "                samples_by_split['test'].extend(samples)\n",
    "            \n",
    "            all_samples.extend(samples)\n",
    "            print(f\"  [OK] Loaded {len(samples)} samples from {split}\")\n",
    "        except Exception as e:\n",
    "            print(f\"  [WARNING] Failed to load {split}: {e}\")\n",
    "    \n",
    "    # Special handling for VLSP2020 (only has train split)\n",
    "    if dataset_name == 'VLSP2020' and all_samples:\n",
    "        print(f\"  [INFO] VLSP2020 only has train split - creating train/val/test splits (70/15/15)\")\n",
    "        np.random.seed(42)\n",
    "        indices = np.random.permutation(len(all_samples))\n",
    "        train_end = int(len(all_samples) * 0.7)\n",
    "        val_end = int(len(all_samples) * 0.85)\n",
    "        \n",
    "        samples_by_split['train'] = [all_samples[i] for i in indices[:train_end]]\n",
    "        samples_by_split['val'] = [all_samples[i] for i in indices[train_end:val_end]]\n",
    "        samples_by_split['test'] = [all_samples[i] for i in indices[val_end:]]\n",
    "        \n",
    "        print(f\"  [OK] Split into: train={len(samples_by_split['train'])}, \"\n",
    "              f\"val={len(samples_by_split['val'])}, test={len(samples_by_split['test'])}\")\n",
    "    \n",
    "    return samples_by_split\n",
    "\n",
    "# MODEL EVALUATOR MODULE\n",
    "from transformers import Wav2Vec2Processor, Wav2Vec2ForCTC\n",
    "import librosa\n",
    "\n",
    "class Wav2Vec2Model:\n",
    "    def __init__(self, model_id: str):\n",
    "        self.model_id = model_id\n",
    "        self.device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "        self.processor = None\n",
    "        self.model = None\n",
    "    \n",
    "    def load_model(self):\n",
    "        print(f\"[INFO] Loading {self.model_id}...\")\n",
    "        try:\n",
    "            self.processor = Wav2Vec2Processor.from_pretrained(self.model_id)\n",
    "            self.model = Wav2Vec2ForCTC.from_pretrained(self.model_id)\n",
    "            self.model.to(self.device)\n",
    "            self.model.eval()\n",
    "            print(f\"[OK] Model loaded on {self.device}\")\n",
    "        except Exception as e:\n",
    "            print(f\"[ERROR] Failed to load model: {e}\")\n",
    "            raise\n",
    "    \n",
    "    def transcribe(self, audio_path: str) -> str:\n",
    "        try:\n",
    "            audio, sr = librosa.load(audio_path, sr=16000)\n",
    "            input_values = self.processor(audio, sampling_rate=16000, return_tensors=\"pt\").input_values.to(self.device)\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                logits = self.model(input_values).logits\n",
    "            predicted_ids = torch.argmax(logits, dim=-1)\n",
    "            transcription = self.processor.batch_decode(predicted_ids)[0]\n",
    "            return transcription.strip().lower()\n",
    "        except Exception as e:\n",
    "            print(f\"[ERROR] Transcription failed: {e}\")\n",
    "            return \"\"\n",
    "\n",
    "print('[OK] All helper functions loaded successfully!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "MODELS_TO_TEST = [\n",
    "    'anuragshas/wav2vec2-large-xlsr-53-vietnamese',\n",
    "    'nguyenvulebinh/wav2vec2-base-vietnamese-250h',\n",
    "]\n",
    "\n",
    "DATASETS_TO_TEST = [\n",
    "    'ViMD',\n",
    "    # 'BUD500',\n",
    "    # 'LSVSC',\n",
    "    # 'VLSP2020',\n",
    "    # 'VietMed',\n",
    "]\n",
    "\n",
    "# Set to None to use FULL datasets, or set a number for quick testing\n",
    "MAX_SAMPLES_PER_SPLIT = None  # None = use all data\n",
    "\n",
    "TIMESTAMP = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "OUTPUT_CSV = f\"/content/wav2vec2_results_{TIMESTAMP}.csv\"\n",
    "\n",
    "print(f'[CONFIG] Models: {len(MODELS_TO_TEST)}')\n",
    "print(f'[CONFIG] Datasets: {len(DATASETS_TO_TEST)}')\n",
    "print(f'[CONFIG] Samples per split: {MAX_SAMPLES_PER_SPLIT or \"ALL (full dataset)\"}')\n",
    "print(f'[CONFIG] Output: {OUTPUT_CSV}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Load Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets_loaded = {}\n",
    "\n",
    "for dataset_name in DATASETS_TO_TEST:\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Loading dataset: {dataset_name}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    try:\n",
    "        splits = load_huggingface_dataset(dataset_name, max_samples=MAX_SAMPLES_PER_SPLIT)\n",
    "        datasets_loaded[dataset_name] = splits\n",
    "        \n",
    "        print(f\"\\n[OK] {dataset_name} loaded:\")\n",
    "        print(f\"  - Train: {len(splits['train'])} samples\")\n",
    "        print(f\"  - Val: {len(splits['val'])} samples\")\n",
    "        print(f\"  - Test: {len(splits['test'])} samples\")\n",
    "    except Exception as e:\n",
    "        print(f\"[ERROR] Failed to load {dataset_name}: {e}\")\n",
    "        datasets_loaded[dataset_name] = {'train': [], 'val': [], 'test': []}\n",
    "\n",
    "print(f\"\\n[OK] Successfully loaded {len(datasets_loaded)} datasets\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Run Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []\n",
    "metrics_calculator = ASRMetrics()\n",
    "total_start_time = time.time()\n",
    "\n",
    "for model_id in MODELS_TO_TEST:\n",
    "    print(f\"\\n\\n{'='*70}\")\n",
    "    print(f\"EVALUATING MODEL: {model_id}\")\n",
    "    print(f\"{'='*70}\\n\")\n",
    "    \n",
    "    try:\n",
    "        model = Wav2Vec2Model(model_id)\n",
    "        model.load_model()\n",
    "    except Exception as e:\n",
    "        print(f\"[ERROR] Skipping {model_id}: {e}\")\n",
    "        continue\n",
    "    \n",
    "    for dataset_name, splits in datasets_loaded.items():\n",
    "        test_samples = splits['test']\n",
    "        \n",
    "        if not test_samples:\n",
    "            print(f\"[WARNING] No test samples for {dataset_name}, skipping...\")\n",
    "            continue\n",
    "        \n",
    "        print(f\"\\n[INFO] Evaluating on {dataset_name} ({len(test_samples)} samples)...\")\n",
    "        \n",
    "        references = []\n",
    "        hypotheses = []\n",
    "        audio_durations = []\n",
    "        processing_times = []\n",
    "        \n",
    "        for sample in tqdm(test_samples, desc=f\"{dataset_name}\", leave=False):\n",
    "            try:\n",
    "                with RTFTimer() as timer:\n",
    "                    hypothesis = model.transcribe(sample.audio_path)\n",
    "                \n",
    "                references.append(sample.transcription)\n",
    "                hypotheses.append(hypothesis)\n",
    "                audio_durations.append(sample.duration)\n",
    "                processing_times.append(timer.elapsed_time)\n",
    "            except Exception as e:\n",
    "                continue\n",
    "        \n",
    "        if references and hypotheses:\n",
    "            metrics = metrics_calculator.calculate_all_metrics(references, hypotheses)\n",
    "            total_audio_duration = sum(audio_durations)\n",
    "            total_processing_time = sum(processing_times)\n",
    "            rtf = total_processing_time / total_audio_duration if total_audio_duration > 0 else 0\n",
    "            \n",
    "            result = {\n",
    "                'model': model_id,\n",
    "                'dataset': dataset_name,\n",
    "                'samples_processed': len(references),\n",
    "                'WER': metrics['wer'],\n",
    "                'CER': metrics['cer'],\n",
    "                'MER': metrics['mer'],\n",
    "                'WIL': metrics['wil'],\n",
    "                'WIP': metrics['wip'],\n",
    "                'SER': metrics['ser'],\n",
    "                'RTF': rtf,\n",
    "                'insertions': metrics['insertions'],\n",
    "                'deletions': metrics['deletions'],\n",
    "                'substitutions': metrics['substitutions'],\n",
    "                'total_audio_duration': total_audio_duration,\n",
    "                'total_processing_time': total_processing_time\n",
    "            }\n",
    "            results.append(result)\n",
    "            print(f\"  [OK] WER: {metrics['wer']:.4f} | CER: {metrics['cer']:.4f} | RTF: {rtf:.4f}\")\n",
    "    \n",
    "    del model\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "total_time = time.time() - total_start_time\n",
    "print(f\"\\n\\n{'='*70}\")\n",
    "print(f\"EVALUATION COMPLETE! Time: {total_time/60:.2f} min\")\n",
    "print(f\"{'='*70}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6-8: Results, Visualizations & Export\n",
    "\n",
    "(Same as notebooks 01-02)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "results_df = pd.DataFrame(results)\n",
    "print(results_df[['model', 'dataset', 'WER', 'CER', 'RTF']].to_string(index=False))\n",
    "\n",
    "# Save and download\n",
    "results_df.to_csv(OUTPUT_CSV, index=False)\n",
    "print(f\"\\n[OK] Results saved to: {OUTPUT_CSV}\")\n",
    "\n",
    "try:\n",
    "    from google.colab import files\n",
    "    files.download(OUTPUT_CSV)\n",
    "except:\n",
    "    print(\"[INFO] File saved locally\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
