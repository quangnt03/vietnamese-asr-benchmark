{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "colab-header"
   },
   "source": [
    "# Vietnamese ASR Evaluation - HuggingFace Integration\n",
    "\n",
    "This notebook demonstrates how to use HuggingFace datasets with the Vietnamese ASR evaluation pipeline.\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/quangnt03/vietnamese-asr-benchmark/blob/main/huggingface_integration_example.ipynb)\n",
    "\n",
    "**Note**: This notebook is compatible with Google Colab. The setup cells below will automatically install dependencies and clone the repository."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "setup-header"
   },
   "source": [
    "## [CONFIG] Google Colab Setup\n",
    "\n",
    "Run these cells if you're using Google Colab. They will:\n",
    "1. Detect if running on Colab\n",
    "2. Clone the repository\n",
    "3. Install all required dependencies\n",
    "4. Set up the environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "check-colab"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[OK] Running locally\n"
     ]
    }
   ],
   "source": [
    "# Check if running on Google Colab\n",
    "try:\n",
    "    import google.colab\n",
    "    IN_COLAB = True\n",
    "    print(\"[OK] Running on Google Colab\")\n",
    "except:\n",
    "    IN_COLAB = False\n",
    "    print(\"[OK] Running locally\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "clone-repo"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping Colab setup (running locally)\n"
     ]
    }
   ],
   "source": [
    "# Clone repository and install dependencies (Colab only)\n",
    "if IN_COLAB:\n",
    "    print(\"Setting up environment for Google Colab...\\n\")\n",
    "    \n",
    "    # Clone the repository\n",
    "    print(\" Cloning repository...\")\n",
    "    !git clone https://github.com/quangnt03/vietnamese-asr-benchmark.git\n",
    "    \n",
    "    # Change to repository directory\n",
    "    import os\n",
    "    os.chdir('vietnamese-asr-benchmark')\n",
    "    print(f\"[OK] Changed directory to: {os.getcwd()}\")\n",
    "    \n",
    "    # Install dependencies\n",
    "    print(\"\\n[PACKAGE] Installing dependencies...\")\n",
    "    !pip install -q -r requirements.txt\n",
    "    \n",
    "    print(\"\\n[OK] Setup complete! You can now run the notebook.\")\n",
    "else:\n",
    "    print(\"Skipping Colab setup (running locally)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "verify-setup"
   },
   "outputs": [],
   "source": [
    "# Verify installation\n",
    "if IN_COLAB:\n",
    "    import sys\n",
    "    from pathlib import Path\n",
    "    \n",
    "    print(\"Verifying installation...\")\n",
    "    print(f\"Python version: {sys.version.split()[0]}\")\n",
    "    print(f\"Working directory: {Path.cwd()}\")\n",
    "    \n",
    "    # Check if key files exist in new structure\n",
    "    key_files = ['src/metrics.py', 'src/dataset_loader.py', \n",
    "                 'src/model_evaluator.py', 'src/visualization.py',\n",
    "                 'scripts/fetch_datasets.py', \"configs/dataset_profile.json\"]\n",
    "    for file in key_files:\n",
    "        if Path(file).exists():\n",
    "            print(f\"  [OK] {file}\")\n",
    "        else:\n",
    "            print(f\"  [FAILED] {file} - NOT FOUND\")\n",
    "    \n",
    "    print(\"\\n[OK] Verification complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "setup-imports"
   },
   "source": [
    "## 1. Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "imports"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pc/miniconda3/envs/lactech-stt/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "\n",
    "# Add src and scripts directories to path for local execution\n",
    "sys.path.insert(0, str(Path.cwd().parent))\n",
    "\n",
    "# Import custom modules\n",
    "from src.metrics import ASRMetrics, format_metrics_report\n",
    "from src.dataset_loader import DatasetManager, HuggingFaceDatasetLoader\n",
    "from src.model_evaluator import ModelEvaluator, ModelFactory\n",
    "from src.visualization import ASRVisualizer\n",
    "from scripts.fetch_datasets import HuggingFaceDatasetFetcher\n",
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "list-datasets-header"
   },
   "source": [
    "## 2. List Available HuggingFace Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/media/pc/shared'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "# Add src and scripts directories to path for local execution\n",
    "os.chdir(Path.cwd().parent)\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "list-datasets"
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "Dataset configuration file not found: configs/dataset_profile.json",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mFileNotFoundError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[11]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Add src and scripts directories to path for local execution\u001b[39;00m\n\u001b[32m      2\u001b[39m sys.path.insert(\u001b[32m0\u001b[39m, \u001b[38;5;28mstr\u001b[39m(Path.cwd().parent))\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m fetcher = \u001b[43mHuggingFaceDatasetFetcher\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m./data/huggingface_cache\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig_file\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mconfigs/dataset_profile.json\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m      6\u001b[39m \u001b[38;5;66;03m# List available datasets\u001b[39;00m\n\u001b[32m      7\u001b[39m fetcher.list_available_datasets()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/media/pc/shared/vietnamese_asr_benchmark/scripts/fetch_datasets.py:42\u001b[39m, in \u001b[36mHuggingFaceDatasetFetcher.__init__\u001b[39m\u001b[34m(self, cache_dir, config_file)\u001b[39m\n\u001b[32m     40\u001b[39m config_path = Path(config_file)\n\u001b[32m     41\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m config_path.exists():\n\u001b[32m---> \u001b[39m\u001b[32m42\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mDataset configuration file not found: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconfig_file\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     44\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(config_path, \u001b[33m'\u001b[39m\u001b[33mr\u001b[39m\u001b[33m'\u001b[39m, encoding=\u001b[33m'\u001b[39m\u001b[33mutf-8\u001b[39m\u001b[33m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[32m     45\u001b[39m     \u001b[38;5;28mself\u001b[39m.AVAILABLE_DATASETS = json.load(f)\n",
      "\u001b[31mFileNotFoundError\u001b[39m: Dataset configuration file not found: configs/dataset_profile.json"
     ]
    }
   ],
   "source": [
    "# Add src and scripts directories to path for local execution\n",
    "sys.path.insert(0, str(Path.cwd().parent))\n",
    "\n",
    "fetcher = HuggingFaceDatasetFetcher(cache_dir=\"./data/huggingface_cache\", config_file=\"configs/dataset_profile.json\")\n",
    "\n",
    "# List available datasets\n",
    "fetcher.list_available_datasets()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fetch-single-header"
   },
   "source": [
    "## 3. Fetch Datasets from HuggingFace\n",
    "\n",
    "### Option A: Fetch a single dataset (quick test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fetch-single"
   },
   "outputs": [],
   "source": [
    "# Fetch Common Voice Vietnamese (limited to 10 samples for quick test)\n",
    "datasets = fetcher.fetch_dataset(\n",
    "    dataset_key='common_voice_vi',\n",
    "    splits=['test'],  # Only test split\n",
    "    max_samples=10,\n",
    "    save_to_disk=True\n",
    ")\n",
    "\n",
    "print(f\"\\nFetched {sum(len(ds) for ds in datasets.values())} samples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fetch-multiple-header"
   },
   "source": [
    "### Option B: Fetch multiple datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fetch-multiple"
   },
   "outputs": [],
   "source": [
    "# Fetch multiple datasets (10 samples each for testing)\n",
    "dataset_results = fetcher.fetch_multiple_datasets(\n",
    "    dataset_keys=['common_voice_vi', 'vivos'],\n",
    "    max_samples=10\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "load-hf-header"
   },
   "source": [
    "## 4. Load HuggingFace Datasets with DatasetManager"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "load-hf"
   },
   "outputs": [],
   "source": [
    "# Initialize dataset manager\n",
    "manager = DatasetManager(base_data_dir=\"./data\")\n",
    "\n",
    "# Load HuggingFace datasets\n",
    "datasets = manager.load_all_datasets(\n",
    "    use_huggingface=True,\n",
    "    hf_datasets=['common_voice_vi', 'vivos']\n",
    ")\n",
    "\n",
    "# Get statistics\n",
    "stats_df = manager.get_dataset_statistics()\n",
    "print(\"\\nDataset Statistics:\")\n",
    "display(stats_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "explore-sample-header"
   },
   "source": [
    "## 5. Explore a HuggingFace Dataset Sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "explore-sample"
   },
   "outputs": [],
   "source": [
    "# Get a sample from Common Voice\n",
    "if 'common_voice_vi' in datasets and len(datasets['common_voice_vi']) > 0:\n",
    "    sample = datasets['common_voice_vi'][0]\n",
    "    \n",
    "    print(\"Sample Information:\")\n",
    "    print(f\"  Audio Path: {sample.audio_path}\")\n",
    "    print(f\"  Transcription: {sample.transcription}\")\n",
    "    print(f\"  Duration: {sample.duration:.2f}s\")\n",
    "    print(f\"  Sample Rate: {sample.sample_rate}Hz\")\n",
    "    print(f\"  Dataset: {sample.dataset}\")\n",
    "    print(f\"  Split: {sample.split}\")\n",
    "    \n",
    "    # Check if audio array is available\n",
    "    if sample.metadata and 'audio_array' in sample.metadata:\n",
    "        audio_array = sample.metadata['audio_array']\n",
    "        print(f\"  Audio array shape: {audio_array.shape if hasattr(audio_array, 'shape') else len(audio_array)}\")\n",
    "else:\n",
    "    print(\"No Common Voice dataset loaded. Please fetch it first.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "splits-header"
   },
   "source": [
    "## 6. Prepare Train/Val/Test Splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "splits"
   },
   "outputs": [],
   "source": [
    "# Prepare splits\n",
    "splits = manager.prepare_train_test_splits(\n",
    "    train_ratio=0.7,\n",
    "    val_ratio=0.15,\n",
    "    test_ratio=0.15\n",
    ")\n",
    "\n",
    "# Show split sizes\n",
    "for dataset_name, split_data in splits.items():\n",
    "    print(f\"\\n{dataset_name}:\")\n",
    "    for split_name, samples in split_data.items():\n",
    "        print(f\"  {split_name}: {len(samples)} samples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "models-header"
   },
   "source": [
    "## 7. Load ASR Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "models-list"
   },
   "outputs": [],
   "source": [
    "# List available models\n",
    "print(\"Available models:\")\n",
    "for model_key in ModelFactory.get_available_models():\n",
    "    config = ModelFactory.MODEL_CONFIGS[model_key]\n",
    "    print(f\"  {model_key}: {config.name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "models-load"
   },
   "outputs": [],
   "source": [
    "# Load specific models for evaluation\n",
    "evaluator = ModelEvaluator(\n",
    "    models_to_evaluate=['phowhisper-small']  # Start with one model\n",
    ")\n",
    "\n",
    "evaluator.load_models()\n",
    "models = evaluator.get_loaded_models()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "evaluate-header"
   },
   "source": [
    "## 8. Evaluate a Model on HuggingFace Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "evaluate"
   },
   "outputs": [],
   "source": [
    "from tqdm.notebook import tqdm\n",
    "\n",
    "def evaluate_on_hf_dataset(model, samples, max_samples=5):\n",
    "    \"\"\"\n",
    "    Evaluate a model on HuggingFace dataset samples.\n",
    "    \"\"\"\n",
    "    references = []\n",
    "    hypotheses = []\n",
    "    \n",
    "    for sample in tqdm(samples[:max_samples], desc=\"Transcribing\"):\n",
    "        # The model can handle AudioSample objects with audio arrays\n",
    "        hypothesis = model.transcribe(sample)\n",
    "        references.append(sample.transcription)\n",
    "        hypotheses.append(hypothesis)\n",
    "        \n",
    "        # Show example\n",
    "        if len(references) == 1:\n",
    "            print(f\"\\nExample:\")\n",
    "            print(f\"  Reference:  {sample.transcription}\")\n",
    "            print(f\"  Hypothesis: {hypothesis}\")\n",
    "    \n",
    "    # Calculate metrics\n",
    "    calculator = ASRMetrics()\n",
    "    metrics = calculator.calculate_batch_metrics(references, hypotheses)\n",
    "    \n",
    "    return metrics, references, hypotheses\n",
    "\n",
    "# Run evaluation\n",
    "if models and splits and 'common_voice_vi' in splits:\n",
    "    model_name = list(models.keys())[0]\n",
    "    test_samples = splits['common_voice_vi']['test']\n",
    "    \n",
    "    print(f\"\\nEvaluating {model_name} on Common Voice Vietnamese (test set)...\\n\")\n",
    "    metrics, refs, hyps = evaluate_on_hf_dataset(\n",
    "        models[model_name],\n",
    "        test_samples,\n",
    "        max_samples=5\n",
    "    )\n",
    "    \n",
    "    print(format_metrics_report(metrics, f\"{model_name} on Common Voice VI\"))\n",
    "else:\n",
    "    print(\"Models or datasets not loaded. Please run previous cells.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "compare-header"
   },
   "source": [
    "## 9. Compare Multiple HuggingFace Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "compare"
   },
   "outputs": [],
   "source": [
    "# Evaluate across all loaded HuggingFace datasets\n",
    "if models:\n",
    "    model_name = list(models.keys())[0]\n",
    "    model = models[model_name]\n",
    "    \n",
    "    results = []\n",
    "    \n",
    "    for dataset_name, split_data in splits.items():\n",
    "        test_samples = split_data['test'][:5]  # Limit to 5 samples\n",
    "        \n",
    "        print(f\"\\nEvaluating on {dataset_name}...\")\n",
    "        metrics, _, _ = evaluate_on_hf_dataset(model, test_samples, max_samples=5)\n",
    "        \n",
    "        results.append({\n",
    "            'Dataset': dataset_name,\n",
    "            'Model': model_name,\n",
    "            'WER': metrics['wer'],\n",
    "            'CER': metrics['cer'],\n",
    "            'MER': metrics['mer'],\n",
    "            'SER': metrics['ser']\n",
    "        })\n",
    "    \n",
    "    # Create comparison DataFrame\n",
    "    results_df = pd.DataFrame(results)\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"Comparison Across HuggingFace Datasets\")\n",
    "    print(\"=\"*70)\n",
    "    display(results_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "visualize-header"
   },
   "source": [
    "## 10. Visualize Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "visualize"
   },
   "outputs": [],
   "source": [
    "# Create visualizations\n",
    "if 'results_df' in locals() and len(results_df) > 0:\n",
    "    visualizer = ASRVisualizer(output_dir=\"./hf_evaluation_plots\")\n",
    "    \n",
    "    # Create metric comparison plot\n",
    "    visualizer.plot_metric_comparison(results_df, metric='wer')\n",
    "    visualizer.plot_metric_comparison(results_df, metric='cer')\n",
    "    \n",
    "    print(\"\\nPlots saved to: ./hf_evaluation_plots/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "direct-hf-header"
   },
   "source": [
    "## 11. Using HuggingFace Datasets Directly (Advanced)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "direct-hf"
   },
   "outputs": [],
   "source": [
    "# Direct loading from HuggingFace (without our wrapper)\n",
    "from datasets import load_dataset, Audio\n",
    "\n",
    "# Load Common Voice Vietnamese directly\n",
    "print(\"Loading Common Voice Vietnamese directly from HuggingFace...\")\n",
    "raw_dataset = load_dataset(\n",
    "    \"mozilla-foundation/common_voice_13_0\",\n",
    "    \"vi\",\n",
    "    split=\"test\",\n",
    "    trust_remote_code=True\n",
    ")\n",
    "\n",
    "# Take a small sample\n",
    "raw_dataset = raw_dataset.select(range(min(5, len(raw_dataset))))\n",
    "\n",
    "# Cast audio to 16kHz\n",
    "raw_dataset = raw_dataset.cast_column(\"audio\", Audio(sampling_rate=16000))\n",
    "\n",
    "print(f\"\\nLoaded {len(raw_dataset)} samples\")\n",
    "print(f\"\\nFeatures: {raw_dataset.features}\")\n",
    "\n",
    "# Show first sample\n",
    "if len(raw_dataset) > 0:\n",
    "    sample = raw_dataset[0]\n",
    "    print(f\"\\nFirst sample:\")\n",
    "    print(f\"  Sentence: {sample['sentence']}\")\n",
    "    print(f\"  Audio array shape: {len(sample['audio']['array'])}\")\n",
    "    print(f\"  Sample rate: {sample['audio']['sampling_rate']}Hz\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "export-header"
   },
   "source": [
    "## 12. Export Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "export"
   },
   "outputs": [],
   "source": [
    "# Export evaluation results\n",
    "if 'results_df' in locals():\n",
    "    # CSV\n",
    "    results_df.to_csv('./hf_evaluation_results.csv', index=False)\n",
    "    print(\"[OK] Results saved to: ./hf_evaluation_results.csv\")\n",
    "    \n",
    "    # JSON\n",
    "    results_df.to_json('./hf_evaluation_results.json', orient='records', indent=2)\n",
    "    print(\"[OK] Results saved to: ./hf_evaluation_results.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "summary"
   },
   "source": [
    "## Summary\n",
    "\n",
    "This notebook demonstrated:\n",
    "\n",
    "1. [OK] Listing available Vietnamese datasets on HuggingFace\n",
    "2. [OK] Fetching datasets from HuggingFace Hub\n",
    "3. [OK] Loading HuggingFace datasets with our pipeline\n",
    "4. [OK] Exploring dataset samples\n",
    "5. [OK] Preparing train/val/test splits\n",
    "6. [OK] Evaluating ASR models on HuggingFace datasets\n",
    "7. [OK] Comparing performance across datasets\n",
    "8. [OK] Visualizing results\n",
    "9. [OK] Direct HuggingFace datasets usage\n",
    "10. [OK] Exporting results\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "**For CLI evaluation with HuggingFace datasets:**\n",
    "```bash\n",
    "# Fetch datasets first\n",
    "python fetch_datasets.py --fetch common_voice_vi vivos --max-samples 100\n",
    "\n",
    "# Run evaluation\n",
    "python main_evaluation.py \\\n",
    "    --use-huggingface \\\n",
    "    --hf-datasets common_voice_vi vivos \\\n",
    "    --models phowhisper-small whisper-small\n",
    "```\n",
    "\n",
    "**For full-scale evaluation:**\n",
    "```bash\n",
    "# Fetch all datasets (no sample limit)\n",
    "python fetch_datasets.py --fetch-all\n",
    "\n",
    "# Run complete evaluation\n",
    "python main_evaluation.py --use-huggingface\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "lactech-stt",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
