{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Cross-Model Comparison & Best Model Selection\n\nThis notebook aggregates results from all model families and performs comprehensive comparison:\n\n## Model Families Compared:\n1. **PhoWhisper** (Vietnamese-optimized Whisper variants)\n2. **OpenAI Whisper** (Original multilingual models)\n3. **Wav2Vec2-XLSR** (Vietnamese fine-tuned CTC models)\n4. **Wav2Vn** (Vietnamese ASR model - Note: uses mock transcription if not publicly available)\n\n## Analysis Performed:\n- Overall best model by WER/CER\n- Best model per dataset\n- Speed vs accuracy trade-offs (RTF vs WER)\n- Statistical significance testing\n- Production deployment recommendations\n\n**Prerequisites**: Run notebooks 01, 02, 03, and 04 first  \n**Compatible with**: Local & Google Colab  \n**Report output**: `/docs/reports/`"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Environment Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Cell 1: Environment detection and setup\n%load_ext autoreload\n%autoreload 3\nimport sys\nfrom pathlib import Path\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# Import notebook utilities\ntry:\n    from src.notebook_utils import (\n        detect_environment,\n        setup_paths,\n        print_environment_info,\n        ReportGenerator\n    )\nexcept ImportError:\n    notebook_dir = Path.cwd()\n    if notebook_dir.name != 'notebooks':\n        sys.path.insert(0, str(notebook_dir.parent))\n    from src.notebook_utils import (\n        detect_environment,\n        setup_paths,\n        print_environment_info,\n        ReportGenerator\n    )\n\n# Detect environment\nENV = detect_environment()\nprint(f\"[INFO] Running in: {ENV}\")\n\n# Setup paths\nPATHS = setup_paths()\nprint(f\"\\n[OK] Project root: {PATHS['project_root']}\")\nprint(f\"[OK] Results directory: {PATHS['output_dir']}\")\nprint(f\"[OK] Reports directory: {PATHS['reports_dir']}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 2: Import libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime\n",
    "from glob import glob\n",
    "import json\n",
    "from scipy import stats\n",
    "\n",
    "# Set plotting style\n",
    "sns.set_style(\"whitegrid\")\n",
    "plt.rcParams['figure.figsize'] = (14, 8)\n",
    "plt.rcParams['font.size'] = 11\n",
    "\n",
    "print(\"[OK] All libraries imported successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load Results from Previous Evaluations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 3: Configuration\n",
    "TIMESTAMP = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "OUTPUT_DIR = PATHS['output_dir'] / f\"cross_comparison_{TIMESTAMP}\"\n",
    "OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "plots_dir = OUTPUT_DIR / \"plots\"\n",
    "plots_dir.mkdir(exist_ok=True)\n",
    "\n",
    "print(f\"[CONFIG] Output directory: {OUTPUT_DIR}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Cell 4: Find and load all result files\nresults_dir = PATHS['output_dir']\n\n# Search for CSV result files\nphowhisper_csvs = list(results_dir.glob('phowhisper_*/phowhisper_results_*.csv'))\nwhisper_csvs = list(results_dir.glob('whisper_*/whisper_results_*.csv'))\nwav2vec2_csvs = list(results_dir.glob('wav2vec2_*/wav2vec2_results_*.csv'))\nwav2vn_csvs = list(results_dir.glob('wav2vn_*/wav2vn_results_*.csv'))\n\nprint(f\"[INFO] Found result files:\")\nprint(f\"  - PhoWhisper: {len(phowhisper_csvs)} files\")\nprint(f\"  - Whisper: {len(whisper_csvs)} files\")\nprint(f\"  - Wav2Vec2: {len(wav2vec2_csvs)} files\")\nprint(f\"  - Wav2Vn: {len(wav2vn_csvs)} files\")\n\n# Load the most recent results for each model family\nall_results = []\n\nif phowhisper_csvs:\n    latest_pho = sorted(phowhisper_csvs)[-1]\n    df_pho = pd.read_csv(latest_pho)\n    df_pho['model_family'] = 'PhoWhisper'\n    all_results.append(df_pho)\n    print(f\"[OK] Loaded PhoWhisper: {latest_pho.name}\")\n\nif whisper_csvs:\n    latest_whi = sorted(whisper_csvs)[-1]\n    df_whi = pd.read_csv(latest_whi)\n    df_whi['model_family'] = 'Whisper'\n    all_results.append(df_whi)\n    print(f\"[OK] Loaded Whisper: {latest_whi.name}\")\n\nif wav2vec2_csvs:\n    latest_w2v = sorted(wav2vec2_csvs)[-1]\n    df_w2v = pd.read_csv(latest_w2v)\n    df_w2v['model_family'] = 'Wav2Vec2'\n    all_results.append(df_w2v)\n    print(f\"[OK] Loaded Wav2Vec2: {latest_w2v.name}\")\n\nif wav2vn_csvs:\n    latest_w2vn = sorted(wav2vn_csvs)[-1]\n    df_w2vn = pd.read_csv(latest_w2vn)\n    df_w2vn['model_family'] = 'Wav2Vn'\n    all_results.append(df_w2vn)\n    print(f\"[OK] Loaded Wav2Vn: {latest_w2vn.name}\")\n    print(f\"[NOTE] Wav2Vn uses mock transcription - results are not real\")\n\nif not all_results:\n    print(\"\\n[WARNING] No result files found!\")\n    print(\"[INFO] Please run notebooks 01-04 first to generate results.\")\nelse:\n    # Combine all results\n    combined_df = pd.concat(all_results, ignore_index=True)\n    print(f\"\\n[OK] Combined results: {len(combined_df)} rows\")\n    print(f\"[INFO] Model families: {combined_df['model_family'].unique().tolist()}\")\n    print(f\"[INFO] Datasets: {combined_df['dataset'].unique().tolist()}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 5: Display combined results\n",
    "if all_results:\n",
    "    print(\"[INFO] Combined Evaluation Results:\")\n",
    "    display_cols = ['model_family', 'model', 'dataset', 'WER', 'CER', 'RTF', 'samples_processed']\n",
    "    print(combined_df[display_cols].to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Overall Best Model Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 6: Find best models overall\n",
    "if all_results:\n",
    "    # Best by WER\n",
    "    best_wer_idx = combined_df['WER'].idxmin()\n",
    "    best_wer = combined_df.loc[best_wer_idx]\n",
    "    \n",
    "    # Best by CER\n",
    "    best_cer_idx = combined_df['CER'].idxmin()\n",
    "    best_cer = combined_df.loc[best_cer_idx]\n",
    "    \n",
    "    # Best by RTF (fastest)\n",
    "    best_rtf_idx = combined_df['RTF'].idxmin()\n",
    "    best_rtf = combined_df.loc[best_rtf_idx]\n",
    "    \n",
    "    print(\"[TARGET] BEST MODELS OVERALL\")\n",
    "    print(\"=\"*60)\n",
    "    print(f\"\\n[1] Best WER:\")\n",
    "    print(f\"    Model: {best_wer['model']}\")\n",
    "    print(f\"    Dataset: {best_wer['dataset']}\")\n",
    "    print(f\"    WER: {best_wer['WER']:.4f}\")\n",
    "    print(f\"    CER: {best_wer['CER']:.4f}\")\n",
    "    print(f\"    RTF: {best_wer['RTF']:.4f}\")\n",
    "    \n",
    "    print(f\"\\n[2] Best CER:\")\n",
    "    print(f\"    Model: {best_cer['model']}\")\n",
    "    print(f\"    Dataset: {best_cer['dataset']}\")\n",
    "    print(f\"    WER: {best_cer['WER']:.4f}\")\n",
    "    print(f\"    CER: {best_cer['CER']:.4f}\")\n",
    "    print(f\"    RTF: {best_cer['RTF']:.4f}\")\n",
    "    \n",
    "    print(f\"\\n[3] Fastest (Best RTF):\")\n",
    "    print(f\"    Model: {best_rtf['model']}\")\n",
    "    print(f\"    Dataset: {best_rtf['dataset']}\")\n",
    "    print(f\"    WER: {best_rtf['WER']:.4f}\")\n",
    "    print(f\"    CER: {best_rtf['CER']:.4f}\")\n",
    "    print(f\"    RTF: {best_rtf['RTF']:.4f}\")\n",
    "    print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 7: Average performance by model family\n",
    "if all_results:\n",
    "    print(\"\\n[CHART] Average Performance by Model Family:\")\n",
    "    family_avg = combined_df.groupby('model_family')[['WER', 'CER', 'MER', 'RTF']].mean()\n",
    "    print(family_avg.to_string())\n",
    "    \n",
    "    print(\"\\n[CHART] Average Performance by Model:\")\n",
    "    model_avg = combined_df.groupby('model')[['WER', 'CER', 'MER', 'RTF']].mean().sort_values('WER')\n",
    "    print(model_avg.to_string())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 8: Best model per dataset\n",
    "if all_results:\n",
    "    print(\"\\n[LIST] Best Model per Dataset (by WER):\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    for dataset in combined_df['dataset'].unique():\n",
    "        dataset_df = combined_df[combined_df['dataset'] == dataset]\n",
    "        best_idx = dataset_df['WER'].idxmin()\n",
    "        best = dataset_df.loc[best_idx]\n",
    "        \n",
    "        print(f\"\\n{dataset}:\")\n",
    "        print(f\"  Model: {best['model']}\")\n",
    "        print(f\"  WER: {best['WER']:.4f} | CER: {best['CER']:.4f} | RTF: {best['RTF']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Comprehensive Visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 9: WER comparison across all models and datasets\n",
    "if all_results:\n",
    "    fig, ax = plt.subplots(figsize=(16, 8))\n",
    "    \n",
    "    # Pivot for grouped bar chart\n",
    "    pivot_wer = combined_df.pivot_table(index='dataset', columns='model', values='WER', aggfunc='mean')\n",
    "    pivot_wer.plot(kind='bar', ax=ax, width=0.8)\n",
    "    \n",
    "    ax.set_title('Word Error Rate (WER) - All Models Comparison', fontsize=16, fontweight='bold')\n",
    "    ax.set_xlabel('Dataset', fontsize=13)\n",
    "    ax.set_ylabel('WER (Lower is Better)', fontsize=13)\n",
    "    ax.legend(title='Model', bbox_to_anchor=(1.02, 1), loc='upper left', fontsize=9)\n",
    "    ax.set_xticklabels(ax.get_xticklabels(), rotation=45, ha='right')\n",
    "    ax.grid(axis='y', alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(plots_dir / 'all_models_wer_comparison.png', dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    print(\"[OK] WER comparison plot saved\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 10: Model family comparison boxplot\n",
    "if all_results:\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "    \n",
    "    # WER\n",
    "    sns.boxplot(data=combined_df, x='model_family', y='WER', ax=axes[0, 0])\n",
    "    axes[0, 0].set_title('WER Distribution by Model Family', fontweight='bold')\n",
    "    axes[0, 0].set_ylabel('WER')\n",
    "    \n",
    "    # CER\n",
    "    sns.boxplot(data=combined_df, x='model_family', y='CER', ax=axes[0, 1])\n",
    "    axes[0, 1].set_title('CER Distribution by Model Family', fontweight='bold')\n",
    "    axes[0, 1].set_ylabel('CER')\n",
    "    \n",
    "    # RTF\n",
    "    sns.boxplot(data=combined_df, x='model_family', y='RTF', ax=axes[1, 0])\n",
    "    axes[1, 0].set_title('RTF Distribution by Model Family', fontweight='bold')\n",
    "    axes[1, 0].set_ylabel('RTF')\n",
    "    axes[1, 0].axhline(y=1.0, color='r', linestyle='--', linewidth=1, label='Real-time')\n",
    "    axes[1, 0].legend()\n",
    "    \n",
    "    # WIP\n",
    "    sns.boxplot(data=combined_df, x='model_family', y='WIP', ax=axes[1, 1])\n",
    "    axes[1, 1].set_title('WIP Distribution by Model Family', fontweight='bold')\n",
    "    axes[1, 1].set_ylabel('WIP (Higher is Better)')\n",
    "    \n",
    "    for ax in axes.flat:\n",
    "        ax.set_xlabel('Model Family')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(plots_dir / 'model_family_distributions.png', dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    print(\"[OK] Model family distribution plot saved\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 11: Speed vs Accuracy trade-off (RTF vs WER)\n",
    "if all_results:\n",
    "    fig, ax = plt.subplots(figsize=(14, 8))\n",
    "    \n",
    "    # Scatter plot with model family colors\n",
    "    for family in combined_df['model_family'].unique():\n",
    "        family_df = combined_df[combined_df['model_family'] == family]\n",
    "        ax.scatter(family_df['RTF'], family_df['WER'], label=family, s=100, alpha=0.6)\n",
    "    \n",
    "    # Add model labels\n",
    "    for idx, row in combined_df.iterrows():\n",
    "        ax.annotate(row['model'].split('/')[-1][:15], \n",
    "                   (row['RTF'], row['WER']), \n",
    "                   fontsize=8, alpha=0.7, \n",
    "                   xytext=(5, 5), textcoords='offset points')\n",
    "    \n",
    "    ax.axvline(x=1.0, color='red', linestyle='--', linewidth=2, alpha=0.5, label='Real-time threshold')\n",
    "    ax.set_xlabel('Real-Time Factor (RTF) - Lower is Faster', fontsize=13)\n",
    "    ax.set_ylabel('Word Error Rate (WER) - Lower is Better', fontsize=13)\n",
    "    ax.set_title('Speed vs Accuracy Trade-off: RTF vs WER', fontsize=16, fontweight='bold')\n",
    "    ax.legend(loc='best')\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Annotate ideal region\n",
    "    ax.text(0.05, 0.95, 'IDEAL\\n(Fast + Accurate)', \n",
    "           transform=ax.transAxes, fontsize=12, \n",
    "           verticalalignment='top', \n",
    "           bbox=dict(boxstyle='round', facecolor='lightgreen', alpha=0.3))\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(plots_dir / 'speed_accuracy_tradeoff.png', dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    print(\"[OK] Speed vs accuracy plot saved\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 12: Comprehensive metrics heatmap\n",
    "if all_results:\n",
    "    fig, ax = plt.subplots(figsize=(18, 12))\n",
    "    \n",
    "    # Create pivot table for heatmap\n",
    "    heatmap_data = combined_df.set_index(['model', 'dataset'])[['WER', 'CER', 'MER', 'WIL', 'WIP', 'SER', 'RTF']]\n",
    "    \n",
    "    sns.heatmap(heatmap_data, annot=True, fmt='.3f', cmap='RdYlGn_r', \n",
    "                cbar_kws={'label': 'Metric Value'}, ax=ax, linewidths=0.5)\n",
    "    \n",
    "    ax.set_title('Comprehensive Metrics Heatmap - All Models & Datasets', fontsize=16, fontweight='bold')\n",
    "    ax.set_xlabel('Metric', fontsize=13)\n",
    "    ax.set_ylabel('Model + Dataset', fontsize=13)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(plots_dir / 'comprehensive_metrics_heatmap.png', dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    print(\"[OK] Comprehensive heatmap saved\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 13: Error type breakdown\n",
    "if all_results and 'insertions' in combined_df.columns:\n",
    "    fig, ax = plt.subplots(figsize=(14, 8))\n",
    "    \n",
    "    # Average error types by model\n",
    "    error_cols = ['insertions', 'deletions', 'substitutions']\n",
    "    error_avg = combined_df.groupby('model')[error_cols].mean()\n",
    "    \n",
    "    error_avg.plot(kind='bar', stacked=True, ax=ax, \n",
    "                   color=['#FF6B6B', '#4ECDC4', '#45B7D1'])\n",
    "    \n",
    "    ax.set_title('Error Type Breakdown by Model', fontsize=16, fontweight='bold')\n",
    "    ax.set_xlabel('Model', fontsize=13)\n",
    "    ax.set_ylabel('Average Error Count', fontsize=13)\n",
    "    ax.legend(title='Error Type', labels=['Insertions', 'Deletions', 'Substitutions'])\n",
    "    ax.set_xticklabels(ax.get_xticklabels(), rotation=45, ha='right')\n",
    "    ax.grid(axis='y', alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(plots_dir / 'error_breakdown.png', dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    print(\"[OK] Error breakdown plot saved\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Statistical Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 14: Statistical significance testing\n",
    "if all_results and len(combined_df['model_family'].unique()) >= 2:\n",
    "    print(\"[INFO] Statistical Significance Testing (ANOVA)\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # ANOVA for WER across model families\n",
    "    families = [group['WER'].values for name, group in combined_df.groupby('model_family')]\n",
    "    f_stat, p_value = stats.f_oneway(*families)\n",
    "    \n",
    "    print(f\"\\nWER across Model Families:\")\n",
    "    print(f\"  F-statistic: {f_stat:.4f}\")\n",
    "    print(f\"  P-value: {p_value:.6f}\")\n",
    "    \n",
    "    if p_value < 0.05:\n",
    "        print(f\"  Result: Statistically significant difference (p < 0.05)\")\n",
    "    else:\n",
    "        print(f\"  Result: No significant difference (p >= 0.05)\")\n",
    "    \n",
    "    # Correlation analysis\n",
    "    print(f\"\\n\\n[CHART] Metric Correlations:\")\n",
    "    metric_cols = ['WER', 'CER', 'MER', 'WIL', 'WIP', 'SER', 'RTF']\n",
    "    correlation = combined_df[metric_cols].corr()\n",
    "    print(correlation.to_string())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Production Recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 15: Production deployment recommendations\n",
    "if all_results:\n",
    "    print(\"[TARGET] PRODUCTION DEPLOYMENT RECOMMENDATIONS\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # Best for accuracy (average WER)\n",
    "    model_wer_avg = combined_df.groupby('model')['WER'].mean().sort_values()\n",
    "    best_accuracy = model_wer_avg.index[0]\n",
    "    best_accuracy_wer = model_wer_avg.iloc[0]\n",
    "    \n",
    "    # Best for speed (average RTF)\n",
    "    model_rtf_avg = combined_df.groupby('model')['RTF'].mean().sort_values()\n",
    "    best_speed = model_rtf_avg.index[0]\n",
    "    best_speed_rtf = model_rtf_avg.iloc[0]\n",
    "    \n",
    "    # Best balanced (WER * RTF score)\n",
    "    model_scores = combined_df.groupby('model').agg({'WER': 'mean', 'RTF': 'mean'})\n",
    "    model_scores['balance_score'] = model_scores['WER'] * model_scores['RTF']\n",
    "    best_balanced = model_scores['balance_score'].idxmin()\n",
    "    balanced_wer = model_scores.loc[best_balanced, 'WER']\n",
    "    balanced_rtf = model_scores.loc[best_balanced, 'RTF']\n",
    "    \n",
    "    print(f\"\\n1. BEST FOR ACCURACY (Lowest WER):\")\n",
    "    print(f\"   Recommendation: {best_accuracy}\")\n",
    "    print(f\"   Average WER: {best_accuracy_wer:.4f}\")\n",
    "    print(f\"   Use case: Offline transcription, high accuracy requirements\")\n",
    "    \n",
    "    print(f\"\\n2. BEST FOR SPEED (Lowest RTF):\")\n",
    "    print(f\"   Recommendation: {best_speed}\")\n",
    "    print(f\"   Average RTF: {best_speed_rtf:.4f}\")\n",
    "    print(f\"   Use case: Real-time transcription, latency-sensitive applications\")\n",
    "    \n",
    "    print(f\"\\n3. BEST BALANCED (Speed + Accuracy):\")\n",
    "    print(f\"   Recommendation: {best_balanced}\")\n",
    "    print(f\"   Average WER: {balanced_wer:.4f}\")\n",
    "    print(f\"   Average RTF: {balanced_rtf:.4f}\")\n",
    "    print(f\"   Use case: General-purpose transcription\")\n",
    "    \n",
    "    # Dataset-specific recommendations\n",
    "    print(f\"\\n4. DATASET-SPECIFIC RECOMMENDATIONS:\")\n",
    "    for dataset in combined_df['dataset'].unique():\n",
    "        dataset_df = combined_df[combined_df['dataset'] == dataset]\n",
    "        best_model = dataset_df.loc[dataset_df['WER'].idxmin(), 'model']\n",
    "        best_wer = dataset_df['WER'].min()\n",
    "        print(f\"   {dataset}: {best_model} (WER: {best_wer:.4f})\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Generate Final Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 16: Save combined results\n",
    "if all_results:\n",
    "    # Save combined CSV\n",
    "    csv_path = OUTPUT_DIR / f\"combined_results_{TIMESTAMP}.csv\"\n",
    "    combined_df.to_csv(csv_path, index=False)\n",
    "    print(f\"[OK] Combined results saved: {csv_path}\")\n",
    "    \n",
    "    # Save summary statistics\n",
    "    summary_stats = {\n",
    "        'overall_best_wer': {\n",
    "            'model': best_wer['model'],\n",
    "            'dataset': best_wer['dataset'],\n",
    "            'wer': float(best_wer['WER']),\n",
    "            'cer': float(best_wer['CER']),\n",
    "            'rtf': float(best_wer['RTF'])\n",
    "        },\n",
    "        'model_family_averages': family_avg.to_dict(),\n",
    "        'recommendations': {\n",
    "            'best_accuracy': best_accuracy,\n",
    "            'best_speed': best_speed,\n",
    "            'best_balanced': best_balanced\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    json_path = OUTPUT_DIR / f\"summary_statistics_{TIMESTAMP}.json\"\n",
    "    with open(json_path, 'w', encoding='utf-8') as f:\n",
    "        json.dump(summary_stats, f, indent=2, ensure_ascii=False)\n",
    "    print(f\"[OK] Summary statistics saved: {json_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 17: Generate comprehensive markdown report\n",
    "if all_results:\n",
    "    report_generator = ReportGenerator(reports_dir=PATHS['reports_dir'])\n",
    "    \n",
    "    report_data = {\n",
    "        'models': combined_df['model'].unique().tolist(),\n",
    "        'datasets': combined_df['dataset'].unique().tolist(),\n",
    "        'metrics_summary': {i: row.to_dict() for i, row in combined_df.iterrows()},\n",
    "        'best_model': {\n",
    "            'model_name': best_wer['model'],\n",
    "            'dataset': best_wer['dataset'],\n",
    "            'WER': best_wer['WER'],\n",
    "            'CER': best_wer['CER'],\n",
    "            'RTF': best_wer['RTF']\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    report_path = report_generator.generate_model_report(\n",
    "        model_family=\"Cross-Model_Comparison\",\n",
    "        results=report_data,\n",
    "        output_filename=f\"Báo_cáo_Tổng_hợp_{TIMESTAMP}.md\"\n",
    "    )\n",
    "    \n",
    "    print(f\"\\n[OK] Comprehensive report generated: {report_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Cell 18: Final summary\nif all_results:\n    print(\"\\n\" + \"=\"*60)\n    print(\"[OK] CROSS-MODEL COMPARISON COMPLETE\")\n    print(\"=\"*60)\n    print(f\"\\n[INFO] Generated outputs:\")\n    print(f\"  1. Combined results CSV: {csv_path}\")\n    print(f\"  2. Summary statistics JSON: {json_path}\")\n    print(f\"  3. Comprehensive report: {report_path}\")\n    print(f\"  4. Visualizations: {plots_dir}/\")\n    print(f\"\\n[NOTE] All files saved in: {OUTPUT_DIR}\")\n    print(f\"[NOTE] Reports saved in: {PATHS['reports_dir']}\")\n    print(\"\\n\" + \"=\"*60)\nelse:\n    print(\"\\n[WARNING] Please run notebooks 01-04 first to generate evaluation results.\")"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}