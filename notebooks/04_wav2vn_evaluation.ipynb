{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Wav2Vn Model Evaluation\n",
    "\n",
    "This notebook evaluates the Wav2Vn model on 5 Vietnamese datasets:\n",
    "- ViMD (Vietnamese Medical Dataset)\n",
    "- BUD500 (VLSP Dataset)\n",
    "- LSVSC (Large-Scale Vietnamese Speech Corpus)\n",
    "- VLSP 2020\n",
    "- VietMed\n",
    "\n",
    "## Wav2Vn Model:\n",
    "- `wav2vn` - Vietnamese ASR model\n",
    "\n",
    "## [WARNING] Important Note\n",
    "\n",
    "**Wav2Vn is not publicly available on HuggingFace Hub.**  \n",
    "This notebook currently uses mock transcription for demonstration purposes.\n",
    "\n",
    "If you have access to the Wav2Vn model:\n",
    "1. Update `src/model_evaluator.py` with the correct model ID\n",
    "2. Implement the actual loading logic in `Wav2VnModel.load_model()`\n",
    "3. Re-run this notebook for real evaluation\n",
    "\n",
    "**Compatible with**: Local & Google Colab  \n",
    "**Report output**: `/docs/reports/`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Environment Setup & Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Cell 1: Environment detection and setup\n%load_ext autoreload\n%autoreload 3\nimport sys\nfrom pathlib import Path\n\n# Import notebook utilities\ntry:\n    from src.notebook_utils import (\n        detect_environment,\n        setup_paths,\n        install_dependencies,\n        print_environment_info,\n        ReportGenerator,\n        create_evaluation_summary\n    )\nexcept ImportError:\n    # If not in notebooks directory, add parent to path\n    notebook_dir = Path.cwd()\n    if notebook_dir.name != 'notebooks':\n        sys.path.insert(0, str(notebook_dir.parent))\n    from src.notebook_utils import (\n        detect_environment,\n        setup_paths,\n        install_dependencies,\n        print_environment_info,\n        ReportGenerator,\n        create_evaluation_summary\n    )\n\n# Detect environment\nENV = detect_environment()\nprint(f\"[INFO] Running in: {ENV}\")\n\n# Install dependencies if needed (mainly for Colab)\ninstall_dependencies(ENV)\n\n# Setup paths\nPATHS = setup_paths()\nprint(f\"\\n[OK] Project root: {PATHS['project_root']}\")\nprint(f\"[OK] Data directory: {PATHS['data_dir']}\")\nprint(f\"[OK] Config file: {PATHS['config_file']}\")\nprint(f\"[OK] Reports directory: {PATHS['reports_dir']}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 2: Print environment info\n",
    "print_environment_info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Cell 3: Import project modules\nimport warnings\nwarnings.filterwarnings('ignore')\n\nimport pandas as pd\nimport numpy as np\nfrom datetime import datetime\nimport time\nfrom tqdm.auto import tqdm\n\n# Import project modules with proper src. prefix for IDE type hints\nfrom src.dataset_loader import DatasetManager, AudioSample\nfrom src.model_evaluator import ModelEvaluator, ModelFactory\nfrom src.metrics import ASRMetrics, RTFTimer\nfrom src.visualization import ASRVisualizer\n\nprint(\"[OK] All modules imported successfully\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 4: Configuration\n",
    "# Model configuration\n",
    "MODEL_FAMILY = \"Wav2Vn\"\n",
    "MODELS_TO_TEST = [\n",
    "    \"wav2vn\",  # Note: This uses mock transcription (not publicly available)\n",
    "]\n",
    "\n",
    "# Dataset configuration\n",
    "DATASETS_TO_TEST = [\n",
    "    \"ViMD\",\n",
    "    \"BUD500\",\n",
    "    \"LSVSC\",\n",
    "    \"VLSP2020\",\n",
    "    \"VietMed\"\n",
    "]\n",
    "\n",
    "# Evaluation configuration\n",
    "MAX_SAMPLES_PER_DATASET = None  # None = all samples, or set to e.g., 50 for quick testing\n",
    "TRAIN_RATIO = 0.7\n",
    "VAL_RATIO = 0.15\n",
    "TEST_RATIO = 0.15\n",
    "\n",
    "# Output configuration\n",
    "TIMESTAMP = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "OUTPUT_DIR = PATHS['output_dir'] / f\"wav2vn_{TIMESTAMP}\"\n",
    "OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(\"[WARNING] Wav2Vn is not publicly available - using mock transcription\")\n",
    "print(f\"[CONFIG] Model family: {MODEL_FAMILY}\")\n",
    "print(f\"[CONFIG] Models to test: {len(MODELS_TO_TEST)}\")\n",
    "print(f\"[CONFIG] Datasets to test: {len(DATASETS_TO_TEST)}\")\n",
    "print(f\"[CONFIG] Max samples per dataset: {MAX_SAMPLES_PER_DATASET or 'All'}\")\n",
    "print(f\"[CONFIG] Output directory: {OUTPUT_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Load Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Cell 5: Initialize dataset manager\ndataset_manager = DatasetManager(config_file=PATHS['config_file'])\nprint(\"[OK] Dataset manager initialized\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Cell 6: Load all datasets\ndatasets_loaded = {}\ndataset_stats = []\n\nfor dataset_name in tqdm(DATASETS_TO_TEST, desc=\"Loading datasets\"):\n    try:\n        # Load dataset\n        samples = dataset_manager.load_dataset(\n            dataset_name=dataset_name\n        )\n        # Get test split\n        test_samples = samples['test']\n        \n        # Limit samples if specified\n        if MAX_SAMPLES_PER_DATASET:\n            test_samples = test_samples[:MAX_SAMPLES_PER_DATASET]\n        \n        datasets_loaded[dataset_name] = test_samples\n        \n        # Collect stats\n        dataset_stats.append({\n            'Dataset': dataset_name,\n            'Total Samples': len(samples['train']) + len(samples['val']) + len(samples['test']),\n            'Test Samples': len(test_samples),\n            'Used Samples': len(test_samples)\n        })\n        \n        print(f\"[OK] {dataset_name}: {len(test_samples)} test samples loaded\")\n    except Exception as e:\n        print(f\"[WARNING] Failed to load {dataset_name}: {e}\")\n        datasets_loaded[dataset_name] = []\n\n# Display stats\nstats_df = pd.DataFrame(dataset_stats)\nprint(\"\\n[INFO] Dataset Statistics:\")\nprint(stats_df.to_string(index=False))"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Initialize Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 7: Initialize model evaluator\n",
    "model_evaluator = ModelEvaluator()\n",
    "metrics_calculator = ASRMetrics()\n",
    "\n",
    "print(\"[OK] Model evaluator and metrics calculator initialized\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Run Evaluation\n",
    "\n",
    "**Note:** Since Wav2Vn uses mock transcription, the results will be simulated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 8: Main evaluation loop\n",
    "results = []\n",
    "total_start_time = time.time()\n",
    "total_samples_processed = 0\n",
    "\n",
    "# Iterate through each model\n",
    "for model_name in MODELS_TO_TEST:\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"[INFO] Evaluating model: {model_name}\")\n",
    "    print(f\"[WARNING] Using mock transcription (Wav2Vn not publicly available)\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    # Load model\n",
    "    try:\n",
    "        model = ModelFactory.create_model(model_name)\n",
    "        model.load_model()\n",
    "        print(f\"[OK] Model loaded (mock implementation)\")\n",
    "    except Exception as e:\n",
    "        print(f\"[ERROR] Failed to load model {model_name}: {e}\")\n",
    "        continue\n",
    "    \n",
    "    # Evaluate on each dataset\n",
    "    for dataset_name, test_samples in datasets_loaded.items():\n",
    "        if not test_samples:\n",
    "            print(f\"[WARNING] Skipping {dataset_name} - no samples\")\n",
    "            continue\n",
    "        \n",
    "        print(f\"\\n[INFO] Testing on {dataset_name} ({len(test_samples)} samples)...\")\n",
    "        \n",
    "        # Prepare for evaluation\n",
    "        references = []\n",
    "        hypotheses = []\n",
    "        audio_durations = []\n",
    "        processing_times = []\n",
    "        \n",
    "        # Process each sample\n",
    "        for sample in tqdm(test_samples, desc=f\"{dataset_name}\", leave=False):\n",
    "            try:\n",
    "                # Transcribe with RTF measurement\n",
    "                with RTFTimer() as timer:\n",
    "                    hypothesis = model.transcribe(sample.audio_path)\n",
    "                \n",
    "                # Store results\n",
    "                references.append(sample.transcription)\n",
    "                hypotheses.append(hypothesis)\n",
    "                \n",
    "                # Get audio duration for RTF calculation\n",
    "                import librosa\n",
    "                duration = librosa.get_duration(path=sample.audio_path)\n",
    "                audio_durations.append(duration)\n",
    "                processing_times.append(timer.elapsed_time)\n",
    "                \n",
    "                total_samples_processed += 1\n",
    "            except Exception as e:\n",
    "                print(f\"[WARNING] Failed to process sample {sample.file_id}: {e}\")\n",
    "                continue\n",
    "        \n",
    "        # Calculate metrics\n",
    "        if references and hypotheses:\n",
    "            metrics = metrics_calculator.calculate_all_metrics(\n",
    "                references=references,\n",
    "                hypotheses=hypotheses\n",
    "            )\n",
    "            \n",
    "            # Calculate RTF\n",
    "            total_audio_duration = sum(audio_durations)\n",
    "            total_processing_time = sum(processing_times)\n",
    "            rtf = total_processing_time / total_audio_duration if total_audio_duration > 0 else 0\n",
    "            \n",
    "            # Store results\n",
    "            result = {\n",
    "                'model': model_name,\n",
    "                'dataset': dataset_name,\n",
    "                'samples_processed': len(references),\n",
    "                'WER': metrics['wer'],\n",
    "                'CER': metrics['cer'],\n",
    "                'MER': metrics['mer'],\n",
    "                'WIL': metrics['wil'],\n",
    "                'WIP': metrics['wip'],\n",
    "                'SER': metrics['ser'],\n",
    "                'RTF': rtf,\n",
    "                'insertions': metrics['insertions'],\n",
    "                'deletions': metrics['deletions'],\n",
    "                'substitutions': metrics['substitutions'],\n",
    "                'total_audio_duration': total_audio_duration,\n",
    "                'total_processing_time': total_processing_time\n",
    "            }\n",
    "            results.append(result)\n",
    "            \n",
    "            print(f\"[OK] WER: {metrics['wer']:.4f} | CER: {metrics['cer']:.4f} | RTF: {rtf:.4f}\")\n",
    "            print(f\"[NOTE] Results are from mock transcription\")\n",
    "        else:\n",
    "            print(f\"[WARNING] No valid results for {dataset_name}\")\n",
    "\n",
    "total_evaluation_time = time.time() - total_start_time\n",
    "\n",
    "print(f\"\\n\\n{'='*60}\")\n",
    "print(f\"[OK] Evaluation completed!\")\n",
    "print(f\"[INFO] Total time: {total_evaluation_time:.2f}s ({total_evaluation_time/60:.2f} minutes)\")\n",
    "print(f\"[INFO] Total samples processed: {total_samples_processed}\")\n",
    "print(f\"[WARNING] Results are from mock transcription - not real Wav2Vn performance\")\n",
    "print(f\"{'='*60}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Results Analysis\n",
    "\n",
    "**Disclaimer:** These results are from mock transcription and do not represent actual Wav2Vn performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 9: Create results DataFrame\n",
    "results_df = pd.DataFrame(results)\n",
    "\n",
    "# Display results\n",
    "print(\"[INFO] Complete Results (Mock Transcription):\")\n",
    "print(results_df.to_string(index=False))\n",
    "\n",
    "# Save to CSV\n",
    "csv_path = OUTPUT_DIR / f\"wav2vn_results_{TIMESTAMP}.csv\"\n",
    "results_df.to_csv(csv_path, index=False)\n",
    "print(f\"\\n[OK] Results saved to: {csv_path}\")\n",
    "print(f\"[NOTE] These are mock results - update model_evaluator.py for real evaluation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 10: Summary statistics\n",
    "print(\"\\n[CHART] Average Performance by Model:\")\n",
    "model_avg = results_df.groupby('model')[['WER', 'CER', 'MER', 'RTF']].mean()\n",
    "print(model_avg.to_string())\n",
    "\n",
    "print(\"\\n[CHART] Average Performance by Dataset:\")\n",
    "dataset_avg = results_df.groupby('dataset')[['WER', 'CER', 'MER', 'RTF']].mean()\n",
    "print(dataset_avg.to_string())\n",
    "\n",
    "print(\"\\n[WARNING] Results are simulated - not actual Wav2Vn performance\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 11: Find best model\n",
    "best_wer_idx = results_df['WER'].idxmin()\n",
    "best_model_info = results_df.loc[best_wer_idx]\n",
    "\n",
    "print(\"[TARGET] Best Model (Lowest WER) - Mock Results:\")\n",
    "print(f\"  Model: {best_model_info['model']}\")\n",
    "print(f\"  Dataset: {best_model_info['dataset']}\")\n",
    "print(f\"  WER: {best_model_info['WER']:.4f}\")\n",
    "print(f\"  CER: {best_model_info['CER']:.4f}\")\n",
    "print(f\"  RTF: {best_model_info['RTF']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Visualizations\n",
    "\n",
    "**Note:** Visualizations based on mock transcription results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 12: Create visualizations\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Set style\n",
    "sns.set_style(\"whitegrid\")\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "\n",
    "# Create plots directory\n",
    "plots_dir = OUTPUT_DIR / \"plots\"\n",
    "plots_dir.mkdir(exist_ok=True)\n",
    "\n",
    "# Initialize visualizer\n",
    "visualizer = ASRVisualizer(output_dir=str(plots_dir))\n",
    "\n",
    "print(\"[OK] Visualizer initialized\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 13: WER comparison plot\n",
    "plt.figure(figsize=(14, 6))\n",
    "pivot_wer = results_df.pivot(index='dataset', columns='model', values='WER')\n",
    "pivot_wer.plot(kind='bar', ax=plt.gca())\n",
    "plt.title('Word Error Rate (WER) - Wav2Vn Model (Mock Results)', fontsize=14, fontweight='bold')\n",
    "plt.xlabel('Dataset', fontsize=12)\n",
    "plt.ylabel('WER (Lower is Better)', fontsize=12)\n",
    "plt.legend(title='Model', bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.tight_layout()\n",
    "plt.savefig(plots_dir / 'wer_comparison.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "print(\"[OK] WER comparison plot saved (mock results)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 14: CER comparison plot\n",
    "plt.figure(figsize=(14, 6))\n",
    "pivot_cer = results_df.pivot(index='dataset', columns='model', values='CER')\n",
    "pivot_cer.plot(kind='bar', ax=plt.gca())\n",
    "plt.title('Character Error Rate (CER) - Wav2Vn Model (Mock Results)', fontsize=14, fontweight='bold')\n",
    "plt.xlabel('Dataset', fontsize=12)\n",
    "plt.ylabel('CER (Lower is Better)', fontsize=12)\n",
    "plt.legend(title='Model', bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.tight_layout()\n",
    "plt.savefig(plots_dir / 'cer_comparison.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "print(\"[OK] CER comparison plot saved (mock results)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Generate Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 15: Generate comprehensive report\n",
    "report_generator = ReportGenerator(reports_dir=PATHS['reports_dir'])\n",
    "\n",
    "# Prepare report data\n",
    "report_data = {\n",
    "    'models': MODELS_TO_TEST,\n",
    "    'datasets': DATASETS_TO_TEST,\n",
    "    'metrics_summary': {i: row.to_dict() for i, row in results_df.iterrows()},\n",
    "    'best_model': {\n",
    "        'model_name': best_model_info['model'],\n",
    "        'dataset': best_model_info['dataset'],\n",
    "        'WER': best_model_info['WER'],\n",
    "        'CER': best_model_info['CER'],\n",
    "        'RTF': best_model_info['RTF']\n",
    "    },\n",
    "    'evaluation_time': total_evaluation_time,\n",
    "    'total_samples': total_samples_processed\n",
    "}\n",
    "\n",
    "# Generate Markdown report\n",
    "report_path = report_generator.generate_model_report(\n",
    "    model_family=MODEL_FAMILY,\n",
    "    results=report_data,\n",
    "    output_filename=f\"Báo_cáo_Wav2Vn_{TIMESTAMP}.md\"\n",
    ")\n",
    "\n",
    "# Save JSON results\n",
    "json_path = report_generator.save_results_json(\n",
    "    results=report_data,\n",
    "    filename=f\"wav2vn_results_{TIMESTAMP}.json\"\n",
    ")\n",
    "\n",
    "print(f\"\\n[OK] Markdown report: {report_path}\")\n",
    "print(f\"[OK] JSON results: {json_path}\")\n",
    "print(f\"[WARNING] Report contains mock results only\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 16: Print evaluation summary\n",
    "summary = create_evaluation_summary(\n",
    "    model_family=MODEL_FAMILY,\n",
    "    models_tested=MODELS_TO_TEST,\n",
    "    datasets_tested=DATASETS_TO_TEST,\n",
    "    total_samples=total_samples_processed,\n",
    "    total_time=total_evaluation_time\n",
    ")\n",
    "print(summary)\n",
    "print(\"\\n[WARNING] Results are from mock transcription\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. How to Enable Real Wav2Vn Evaluation\n",
    "\n",
    "To use the actual Wav2Vn model (when available):\n",
    "\n",
    "### Step 1: Update Model Configuration\n",
    "\n",
    "Edit `src/model_evaluator.py`:\n",
    "\n",
    "```python\n",
    "MODEL_CONFIGS = {\n",
    "    'wav2vn': ModelConfig(\n",
    "        name='Wav2Vn',\n",
    "        model_id='your-org/wav2vn-actual-model-id',  # Update this\n",
    "        model_type='wav2vn'\n",
    "    ),\n",
    "}\n",
    "```\n",
    "\n",
    "### Step 2: Implement Loading Logic\n",
    "\n",
    "Edit the `Wav2VnModel` class:\n",
    "\n",
    "```python\n",
    "class Wav2VnModel(BaseASRModel):\n",
    "    def load_model(self):\n",
    "        # Replace mock implementation with actual loading\n",
    "        from transformers import Wav2Vec2Processor, Wav2Vec2ForCTC\n",
    "        self.processor = Wav2Vec2Processor.from_pretrained(self.config.model_id)\n",
    "        self.model = Wav2Vec2ForCTC.from_pretrained(self.config.model_id)\n",
    "        # ... etc\n",
    "```\n",
    "\n",
    "### Step 3: Re-run This Notebook\n",
    "\n",
    "After updating the code, restart the kernel and re-run all cells."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Export & Conclusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 17: Final summary\n",
    "print(\"[OK] Wav2Vn Evaluation Complete!\")\n",
    "print(\"\\n[INFO] Generated outputs:\")\n",
    "print(f\"  1. Results CSV: {csv_path}\")\n",
    "print(f\"  2. Markdown Report: {report_path}\")\n",
    "print(f\"  3. JSON Results: {json_path}\")\n",
    "print(f\"  4. Visualizations: {plots_dir}/\")\n",
    "print(\"\\n[NOTE] All files are saved in:\")\n",
    "print(f\"  - Results: {OUTPUT_DIR}\")\n",
    "print(f\"  - Reports: {PATHS['reports_dir']}\")\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"[WARNING] IMPORTANT REMINDER\")\n",
    "print(\"=\"*60)\n",
    "print(\"These results are from MOCK TRANSCRIPTION only.\")\n",
    "print(\"Wav2Vn is not publicly available on HuggingFace.\")\n",
    "print(\"\\nTo evaluate the real Wav2Vn model:\")\n",
    "print(\"1. Obtain access to the Wav2Vn model\")\n",
    "print(\"2. Update src/model_evaluator.py with correct model ID\")\n",
    "print(\"3. Implement actual loading logic in Wav2VnModel class\")\n",
    "print(\"4. Re-run this notebook\")\n",
    "print(\"=\"*60)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}