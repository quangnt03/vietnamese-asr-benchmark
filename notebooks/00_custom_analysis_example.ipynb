{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "colab-header"
   },
   "source": "# [TEMPLATE] Custom ASR Model Evaluation\n\nThis is a **reusable template** for evaluating new ASR models on Vietnamese datasets.\n\n## How to Use This Template:\n\n1. **Duplicate this notebook** and rename it (e.g., `06_newmodel_evaluation.ipynb`)\n2. **Update the configuration section** (Cell 4):\n   - Change `MODEL_FAMILY` name\n   - Update `MODELS_TO_TEST` list with your model IDs\n   - Adjust dataset list if needed\n3. **Run all cells** to perform evaluation\n\n## What This Template Evaluates:\n\n- **Datasets**: ViMD, BUD500, LSVSC, VLSP2020, VietMed (customizable)\n- **Metrics**: WER, CER, MER, WIL, WIP, SER, RTF\n- **Outputs**: CSV results, JSON reports, comprehensive visualizations\n\n**Compatible with**: Local & Google Colab  \n**Report output**: `/docs/reports/`\n\n---\n\n## Quick Start:\n\n```python\n# In Cell 4, update these variables:\nMODEL_FAMILY = \"YourModelFamily\"  # e.g., \"Seamless-M4T\", \"MMS\"\nMODELS_TO_TEST = [\n    \"your-org/model-name-1\",\n    \"your-org/model-name-2\",\n]\n```"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "setup-header"
   },
   "source": "## 1. Environment Setup & Dependencies"
  },
  {
   "cell_type": "code",
   "source": "# Google Colab Setup - Run this cell FIRST if using Colab\nimport os\n\n# Detect if running on Google Colab\ntry:\n    import google.colab\n    IN_COLAB = True\n    print(\"[INFO] Running on Google Colab\")\nexcept ImportError:\n    IN_COLAB = False\n    print(\"[INFO] Running locally - skipping Colab setup\")\n\nif IN_COLAB:\n    print(\"\\n[SETUP] Setting up Google Colab environment...\")\n    \n    # Clone repository\n    REPO_URL = \"https://github.com/quangnt03/vietnamese-asr-benchmark.git\"\n    REPO_NAME = \"vietnamese_asr_benchmark\"\n    \n    if not os.path.exists(REPO_NAME):\n        print(f\"[SETUP] Cloning repository from {REPO_URL}...\")\n        !git clone {REPO_URL}\n        print(\"[OK] Repository cloned successfully\")\n    else:\n        print(f\"[INFO] Repository already exists at {REPO_NAME}\")\n    \n    # Change to repository directory\n    os.chdir(REPO_NAME)\n    print(f\"[OK] Changed directory to: {os.getcwd()}\")\n    \n    # Install dependencies\n    print(\"\\n[SETUP] Installing dependencies...\")\n    !pip install -q -r requirements.txt\n    print(\"[OK] Dependencies installed\")\n    \n    print(\"\\n[OK] Google Colab setup complete!\")\n    print(\"[INFO] You can now run the remaining cells\")\nelse:\n    print(\"[INFO] Local environment - ensure you are in the project root directory\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## Google Colab Setup (Run this cell first if using Colab)",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "check-colab"
   },
   "outputs": [],
   "source": "# Cell 1: Environment detection and setup\n%load_ext autoreload\n%autoreload 3\nimport sys\nfrom pathlib import Path\n\n# Import notebook utilities\ntry:\n    from src.notebook_utils import (\n        detect_environment,\n        setup_paths,\n        install_dependencies,\n        print_environment_info,\n        ReportGenerator,\n        create_evaluation_summary\n    )\nexcept ImportError:\n    # If not in notebooks directory, add parent to path\n    notebook_dir = Path.cwd()\n    if notebook_dir.name != 'notebooks':\n        sys.path.insert(0, str(notebook_dir.parent))\n    from src.notebook_utils import (\n        detect_environment,\n        setup_paths,\n        install_dependencies,\n        print_environment_info,\n        ReportGenerator,\n        create_evaluation_summary\n    )\n\n# Detect environment\nENV = detect_environment()\nprint(f\"[INFO] Running in: {ENV}\")\n\n# Install dependencies if needed (mainly for Colab)\ninstall_dependencies(ENV)\n\n# Setup paths\nPATHS = setup_paths()\nprint(f\"\\n[OK] Project root: {PATHS['project_root']}\")\nprint(f\"[OK] Data directory: {PATHS['data_dir']}\")\nprint(f\"[OK] Config file: {PATHS['config_file']}\")\nprint(f\"[OK] Reports directory: {PATHS['reports_dir']}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "clone-repo"
   },
   "outputs": [],
   "source": "# Cell 2: Print environment info\nprint_environment_info()"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "verify-setup"
   },
   "outputs": [],
   "source": "# Cell 3: Import project modules\nimport warnings\nwarnings.filterwarnings('ignore')\n\nimport pandas as pd\nimport numpy as np\nfrom datetime import datetime\nimport time\nfrom tqdm.auto import tqdm\n\n# Import project modules with proper src. prefix for IDE type hints\nfrom src.dataset_loader import DatasetManager, AudioSample\nfrom src.model_evaluator import ModelEvaluator, ModelFactory\nfrom src.metrics import ASRMetrics, RTFTimer\nfrom src.visualization import ASRVisualizer\n\nprint(\"[OK] All modules imported successfully\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "setup-imports"
   },
   "source": "## 2. Configuration\n\n[CUSTOMIZE THIS SECTION] Update the variables below for your model family."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "imports"
   },
   "outputs": [],
   "source": "# Cell 4: Configuration\n# [CUSTOMIZE] Model configuration\nMODEL_FAMILY = \"CustomModel\"  # CHANGE THIS: e.g., \"Seamless-M4T\", \"MMS\", \"Canary\"\nMODELS_TO_TEST = [\n    # [CUSTOMIZE] Add your model IDs here\n    # Examples:\n    # \"facebook/seamless-m4t-large\",\n    # \"facebook/mms-1b-all\",\n    # \"your-org/your-model-name\",\n]\n\n# Dataset configuration\nDATASETS_TO_TEST = [\n    \"ViMD\",\n    \"BUD500\",\n    \"LSVSC\",\n    \"VLSP2020\",\n    \"VietMed\"\n]\n\n# Evaluation configuration\nMAX_SAMPLES_PER_DATASET = None  # None = all samples, or set to e.g., 50 for quick testing\nTRAIN_RATIO = 0.7\nVAL_RATIO = 0.15\nTEST_RATIO = 0.15\n\n# Output configuration\nTIMESTAMP = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\nOUTPUT_DIR = PATHS['output_dir'] / f\"{MODEL_FAMILY.lower().replace(' ', '_')}_{TIMESTAMP}\"\nOUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n\nprint(f\"[CONFIG] Model family: {MODEL_FAMILY}\")\nprint(f\"[CONFIG] Models to test: {len(MODELS_TO_TEST)}\")\nprint(f\"[CONFIG] Datasets to test: {len(DATASETS_TO_TEST)}\")\nprint(f\"[CONFIG] Max samples per dataset: {MAX_SAMPLES_PER_DATASET or 'All'}\")\nprint(f\"[CONFIG] Output directory: {OUTPUT_DIR}\")\n\n# Validation\nif not MODELS_TO_TEST:\n    print(\"\\n[WARNING] No models specified in MODELS_TO_TEST!\")\n    print(\"[INFO] Please update Cell 4 with your model IDs before running evaluation.\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "metrics-header"
   },
   "source": "## 3. Load Datasets"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "metrics-single"
   },
   "outputs": [],
   "source": "# Cell 5: Initialize dataset manager\ndataset_manager = DatasetManager(config_file=PATHS['config_file'])\nprint(\"[OK] Dataset manager initialized\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "metrics-batch"
   },
   "outputs": [],
   "source": "# Cell 6: Load all datasets\ndatasets_loaded = {}\ndataset_stats = []\n\nfor dataset_name in tqdm(DATASETS_TO_TEST, desc=\"Loading datasets\"):\n    try:\n        # Load dataset\n        samples = dataset_manager.load_dataset(\n            dataset_name=dataset_name\n        )\n        # Get test split\n        test_samples = samples['test']\n        \n        # Limit samples if specified\n        if MAX_SAMPLES_PER_DATASET:\n            test_samples = test_samples[:MAX_SAMPLES_PER_DATASET]\n        \n        datasets_loaded[dataset_name] = test_samples\n        \n        # Collect stats\n        dataset_stats.append({\n            'Dataset': dataset_name,\n            'Total Samples': len(samples['train']) + len(samples['val']) + len(samples['test']),\n            'Test Samples': len(test_samples),\n            'Used Samples': len(test_samples)\n        })\n        \n        print(f\"[OK] {dataset_name}: {len(test_samples)} test samples loaded\")\n    except Exception as e:\n        print(f\"[WARNING] Failed to load {dataset_name}: {e}\")\n        datasets_loaded[dataset_name] = []\n\n# Display stats\nstats_df = pd.DataFrame(dataset_stats)\nprint(\"\\n[INFO] Dataset Statistics:\")\nprint(stats_df.to_string(index=False))"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "datasets-header"
   },
   "source": "## 4. Initialize Models"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "datasets-load"
   },
   "outputs": [],
   "source": "# Cell 7: Initialize model evaluator\nmodel_evaluator = ModelEvaluator()\nmetrics_calculator = ASRMetrics()\n\nprint(\"[OK] Model evaluator and metrics calculator initialized\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "datasets-split"
   },
   "outputs": [],
   "source": "## 5. Run Evaluation"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "models-header"
   },
   "source": "# Cell 8: Main evaluation loop\nresults = []\ntotal_start_time = time.time()\ntotal_samples_processed = 0\n\n# Check if models are specified\nif not MODELS_TO_TEST:\n    print(\"[ERROR] No models specified in MODELS_TO_TEST!\")\n    print(\"[INFO] Please update Cell 4 and re-run from there.\")\nelse:\n    # Iterate through each model\n    for model_name in MODELS_TO_TEST:\n        print(f\"\\n{'='*60}\")\n        print(f\"[INFO] Evaluating model: {model_name}\")\n        print(f\"{'='*60}\")\n        \n        # Load model\n        try:\n            model = ModelFactory.create_model(model_name)\n            model.load_model()\n            print(f\"[OK] Model loaded successfully\")\n        except Exception as e:\n            print(f\"[ERROR] Failed to load model {model_name}: {e}\")\n            continue\n        \n        # Evaluate on each dataset\n        for dataset_name, test_samples in datasets_loaded.items():\n            if not test_samples:\n                print(f\"[WARNING] Skipping {dataset_name} - no samples\")\n                continue\n            \n            print(f\"\\n[INFO] Testing on {dataset_name} ({len(test_samples)} samples)...\")\n            \n            # Prepare for evaluation\n            references = []\n            hypotheses = []\n            audio_durations = []\n            processing_times = []\n            \n            # Process each sample\n            for sample in tqdm(test_samples, desc=f\"{dataset_name}\", leave=False):\n                try:\n                    # Transcribe with RTF measurement\n                    with RTFTimer() as timer:\n                        hypothesis = model.transcribe(sample.audio_path)\n                    \n                    # Store results\n                    references.append(sample.transcription)\n                    hypotheses.append(hypothesis)\n                    \n                    # Get audio duration for RTF calculation\n                    import librosa\n                    duration = librosa.get_duration(path=sample.audio_path)\n                    audio_durations.append(duration)\n                    processing_times.append(timer.elapsed_time)\n                    \n                    total_samples_processed += 1\n                except Exception as e:\n                    print(f\"[WARNING] Failed to process sample {sample.file_id}: {e}\")\n                    continue\n            \n            # Calculate metrics\n            if references and hypotheses:\n                metrics = metrics_calculator.calculate_all_metrics(\n                    references=references,\n                    hypotheses=hypotheses\n                )\n                \n                # Calculate RTF\n                total_audio_duration = sum(audio_durations)\n                total_processing_time = sum(processing_times)\n                rtf = total_processing_time / total_audio_duration if total_audio_duration > 0 else 0\n                \n                # Store results\n                result = {\n                    'model': model_name,\n                    'dataset': dataset_name,\n                    'samples_processed': len(references),\n                    'WER': metrics['wer'],\n                    'CER': metrics['cer'],\n                    'MER': metrics['mer'],\n                    'WIL': metrics['wil'],\n                    'WIP': metrics['wip'],\n                    'SER': metrics['ser'],\n                    'RTF': rtf,\n                    'insertions': metrics['insertions'],\n                    'deletions': metrics['deletions'],\n                    'substitutions': metrics['substitutions'],\n                    'total_audio_duration': total_audio_duration,\n                    'total_processing_time': total_processing_time\n                }\n                results.append(result)\n                \n                print(f\"[OK] WER: {metrics['wer']:.4f} | CER: {metrics['cer']:.4f} | RTF: {rtf:.4f}\")\n            else:\n                print(f\"[WARNING] No valid results for {dataset_name}\")\n\n    total_evaluation_time = time.time() - total_start_time\n\n    print(f\"\\n\\n{'='*60}\")\n    print(f\"[OK] Evaluation completed!\")\n    print(f\"[INFO] Total time: {total_evaluation_time:.2f}s ({total_evaluation_time/60:.2f} minutes)\")\n    print(f\"[INFO] Total samples processed: {total_samples_processed}\")\n    print(f\"{'='*60}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "models-list"
   },
   "outputs": [],
   "source": "## 6. Results Analysis"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "models-load"
   },
   "outputs": [],
   "source": "# Cell 9: Create results DataFrame\nif results:\n    results_df = pd.DataFrame(results)\n\n    # Display results\n    print(\"[INFO] Complete Results:\")\n    print(results_df.to_string(index=False))\n\n    # Save to CSV\n    csv_path = OUTPUT_DIR / f\"{MODEL_FAMILY.lower().replace(' ', '_')}_results_{TIMESTAMP}.csv\"\n    results_df.to_csv(csv_path, index=False)\n    print(f\"\\n[OK] Results saved to: {csv_path}\")\nelse:\n    print(\"[WARNING] No results to display. Please check if models were specified and loaded correctly.\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "models-transcribe"
   },
   "outputs": [],
   "source": "# Cell 10: Summary statistics\nif results:\n    print(\"\\n[CHART] Average Performance by Model:\")\n    model_avg = results_df.groupby('model')[['WER', 'CER', 'MER', 'RTF']].mean()\n    print(model_avg.to_string())\n\n    print(\"\\n[CHART] Average Performance by Dataset:\")\n    dataset_avg = results_df.groupby('dataset')[['WER', 'CER', 'MER', 'RTF']].mean()\n    print(dataset_avg.to_string())"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "evaluation-header"
   },
   "source": "# Cell 11: Find best model\nif results:\n    best_wer_idx = results_df['WER'].idxmin()\n    best_model_info = results_df.loc[best_wer_idx]\n\n    print(\"[TARGET] Best Model (Lowest WER):\")\n    print(f\"  Model: {best_model_info['model']}\")\n    print(f\"  Dataset: {best_model_info['dataset']}\")\n    print(f\"  WER: {best_model_info['WER']:.4f}\")\n    print(f\"  CER: {best_model_info['CER']:.4f}\")\n    print(f\"  RTF: {best_model_info['RTF']:.4f}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "evaluation-custom"
   },
   "outputs": [],
   "source": "## 7. Visualizations"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "analysis-header"
   },
   "source": "# Cell 12: Create visualizations\nif results:\n    import matplotlib.pyplot as plt\n    import seaborn as sns\n\n    # Set style\n    sns.set_style(\"whitegrid\")\n    plt.rcParams['figure.figsize'] = (12, 6)\n\n    # Create plots directory\n    plots_dir = OUTPUT_DIR / \"plots\"\n    plots_dir.mkdir(exist_ok=True)\n\n    # Initialize visualizer\n    visualizer = ASRVisualizer(output_dir=str(plots_dir))\n\n    print(\"[OK] Visualizer initialized\")\nelse:\n    print(\"[WARNING] No results to visualize\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "analysis-load"
   },
   "outputs": [],
   "source": "# Cell 13: WER comparison plot\nif results:\n    plt.figure(figsize=(14, 6))\n    pivot_wer = results_df.pivot(index='dataset', columns='model', values='WER')\n    pivot_wer.plot(kind='bar', ax=plt.gca())\n    plt.title(f'Word Error Rate (WER) Comparison - {MODEL_FAMILY} Models', fontsize=14, fontweight='bold')\n    plt.xlabel('Dataset', fontsize=12)\n    plt.ylabel('WER (Lower is Better)', fontsize=12)\n    plt.legend(title='Model', bbox_to_anchor=(1.05, 1), loc='upper left')\n    plt.xticks(rotation=45, ha='right')\n    plt.tight_layout()\n    plt.savefig(plots_dir / 'wer_comparison.png', dpi=300, bbox_inches='tight')\n    plt.show()\n    print(\"[OK] WER comparison plot saved\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "analysis-best"
   },
   "outputs": [],
   "source": "# Cell 14: CER comparison plot\nif results:\n    plt.figure(figsize=(14, 6))\n    pivot_cer = results_df.pivot(index='dataset', columns='model', values='CER')\n    pivot_cer.plot(kind='bar', ax=plt.gca())\n    plt.title(f'Character Error Rate (CER) Comparison - {MODEL_FAMILY} Models', fontsize=14, fontweight='bold')\n    plt.xlabel('Dataset', fontsize=12)\n    plt.ylabel('CER (Lower is Better)', fontsize=12)\n    plt.legend(title='Model', bbox_to_anchor=(1.05, 1), loc='upper left')\n    plt.xticks(rotation=45, ha='right')\n    plt.tight_layout()\n    plt.savefig(plots_dir / 'cer_comparison.png', dpi=300, bbox_inches='tight')\n    plt.show()\n    print(\"[OK] CER comparison plot saved\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "viz-header"
   },
   "source": "# Cell 15: RTF comparison plot\nif results:\n    plt.figure(figsize=(14, 6))\n    pivot_rtf = results_df.pivot(index='dataset', columns='model', values='RTF')\n    pivot_rtf.plot(kind='bar', ax=plt.gca())\n    plt.title(f'Real-Time Factor (RTF) Comparison - {MODEL_FAMILY} Models', fontsize=14, fontweight='bold')\n    plt.xlabel('Dataset', fontsize=12)\n    plt.ylabel('RTF (Lower is Better, <1.0 = Real-time)', fontsize=12)\n    plt.axhline(y=1.0, color='r', linestyle='--', label='Real-time threshold')\n    plt.legend(title='Model', bbox_to_anchor=(1.05, 1), loc='upper left')\n    plt.xticks(rotation=45, ha='right')\n    plt.tight_layout()\n    plt.savefig(plots_dir / 'rtf_comparison.png', dpi=300, bbox_inches='tight')\n    plt.show()\n    print(\"[OK] RTF comparison plot saved\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "viz-prepare"
   },
   "outputs": [],
   "source": "# Cell 16: Heatmap of all metrics\nif results:\n    plt.figure(figsize=(16, 10))\n    heatmap_data = results_df.set_index(['model', 'dataset'])[['WER', 'CER', 'MER', 'WIL', 'WIP', 'SER', 'RTF']]\n    sns.heatmap(heatmap_data, annot=True, fmt='.4f', cmap='RdYlGn_r', cbar_kws={'label': 'Metric Value'})\n    plt.title(f'All Metrics Heatmap - {MODEL_FAMILY} Models', fontsize=14, fontweight='bold')\n    plt.xlabel('Metric', fontsize=12)\n    plt.ylabel('Model + Dataset', fontsize=12)\n    plt.tight_layout()\n    plt.savefig(plots_dir / 'metrics_heatmap.png', dpi=300, bbox_inches='tight')\n    plt.show()\n    print(\"[OK] Metrics heatmap saved\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "viz-create"
   },
   "outputs": [],
   "source": "## 8. Generate Report"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "export-header"
   },
   "source": "# Cell 17: Generate comprehensive report\nif results:\n    report_generator = ReportGenerator(reports_dir=PATHS['reports_dir'])\n\n    # Prepare report data\n    report_data = {\n        'models': MODELS_TO_TEST,\n        'datasets': DATASETS_TO_TEST,\n        'metrics_summary': {i: row.to_dict() for i, row in results_df.iterrows()},\n        'best_model': {\n            'model_name': best_model_info['model'],\n            'dataset': best_model_info['dataset'],\n            'WER': best_model_info['WER'],\n            'CER': best_model_info['CER'],\n            'RTF': best_model_info['RTF']\n        },\n        'evaluation_time': total_evaluation_time,\n        'total_samples': total_samples_processed\n    }\n\n    # Generate Markdown report\n    report_path = report_generator.generate_model_report(\n        model_family=MODEL_FAMILY,\n        results=report_data,\n        output_filename=f\"Báo_cáo_{MODEL_FAMILY.replace(' ', '_')}_{TIMESTAMP}.md\"\n    )\n\n    # Save JSON results\n    json_path = report_generator.save_results_json(\n        results=report_data,\n        filename=f\"{MODEL_FAMILY.lower().replace(' ', '_')}_results_{TIMESTAMP}.json\"\n    )\n\n    print(f\"\\n[OK] Markdown report: {report_path}\")\n    print(f\"[OK] JSON results: {json_path}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "export-results"
   },
   "outputs": [],
   "source": "# Cell 18: Print evaluation summary\nif results:\n    summary = create_evaluation_summary(\n        model_family=MODEL_FAMILY,\n        models_tested=MODELS_TO_TEST,\n        datasets_tested=DATASETS_TO_TEST,\n        total_samples=total_samples_processed,\n        total_time=total_evaluation_time\n    )\n    print(summary)"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "summary"
   },
   "source": "## 9. Export & Conclusion"
  },
  {
   "cell_type": "markdown",
   "source": "## Template Usage Examples\n\n### Example 1: Evaluate Meta's Seamless M4T\n\n```python\n# Cell 4 configuration:\nMODEL_FAMILY = \"Seamless-M4T\"\nMODELS_TO_TEST = [\n    \"facebook/seamless-m4t-large\",\n    \"facebook/seamless-m4t-medium\",\n]\n```\n\n### Example 2: Evaluate Meta's MMS (Massively Multilingual Speech)\n\n```python\n# Cell 4 configuration:\nMODEL_FAMILY = \"MMS\"\nMODELS_TO_TEST = [\n    \"facebook/mms-1b-all\",\n    \"facebook/mms-300m\",\n]\n```\n\n### Example 3: Evaluate NVIDIA Canary\n\n```python\n# Cell 4 configuration:\nMODEL_FAMILY = \"Canary\"\nMODELS_TO_TEST = [\n    \"nvidia/canary-1b\",\n]\n```\n\n### Example 4: Quick Testing (Limited Samples)\n\n```python\n# Cell 4 configuration:\nMODEL_FAMILY = \"YourModel\"\nMODELS_TO_TEST = [\"your-model-id\"]\nMAX_SAMPLES_PER_DATASET = 10  # Test with only 10 samples per dataset\nDATASETS_TO_TEST = [\"ViMD\", \"VLSP2020\"]  # Test on fewer datasets\n```\n\n---\n\n## Tips for Using This Template\n\n1. **Start Small**: Test with `MAX_SAMPLES_PER_DATASET = 10` first to verify everything works\n2. **Monitor Memory**: Large models may require GPU with sufficient VRAM\n3. **Save Frequently**: Results are automatically saved after each evaluation\n4. **Compare Results**: Use notebook `05_cross_model_comparison.ipynb` to compare across model families\n5. **Check Model IDs**: Verify HuggingFace model IDs are correct before running\n6. **Custom Models**: Follow the \"How to Add Custom Models\" section above\n\n---\n\n## Next Steps\n\nAfter evaluating your models:\n\n1. Review results in the generated CSV file\n2. Check visualizations in the `plots/` directory\n3. Read the comprehensive report in `docs/reports/`\n4. Compare with other models using `05_cross_model_comparison.ipynb`\n5. Share your findings or contribute back to the project\n\n---\n\n**Happy Evaluating!**",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": "---\n\n## How to Add Custom Models to the Framework\n\nIf your model is not in the default list, you'll need to add it to `src/model_evaluator.py`:\n\n### Step 1: Add Model Configuration\n\n```python\n# In src/model_evaluator.py, add to MODEL_CONFIGS dict:\n'your-model-key': ModelConfig(\n    name='YourModelName',\n    model_id='your-org/your-model-id',\n    model_type='your-model-type'  # e.g., 'whisper', 'wav2vec2', 'custom'\n),\n```\n\n### Step 2: Create Model Class (if needed)\n\nIf your model type is new, create a class:\n\n```python\nclass YourCustomModel(BaseASRModel):\n    def load_model(self):\n        # Load your model here\n        from transformers import AutoModelForSpeechSeq2Seq, AutoProcessor\n        self.processor = AutoProcessor.from_pretrained(self.config.model_id)\n        self.model = AutoModelForSpeechSeq2Seq.from_pretrained(self.config.model_id)\n        \n    def transcribe(self, audio_path: str) -> str:\n        # Implement transcription logic\n        # Return transcribed text\n        pass\n```\n\n### Step 3: Register in ModelFactory\n\n```python\n# In ModelFactory.create_model(), add your model type:\nelif config.model_type == 'your-model-type':\n    return YourCustomModel(config)\n```\n\n### Step 4: Use in This Notebook\n\n```python\nMODELS_TO_TEST = [\n    \"your-model-key\",  # Use the key from MODEL_CONFIGS\n]\n```",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Cell 19: Final summary\nif results:\n    print(f\"[OK] {MODEL_FAMILY} Evaluation Complete!\")\n    print(\"\\n[INFO] Generated outputs:\")\n    print(f\"  1. Results CSV: {csv_path}\")\n    print(f\"  2. Markdown Report: {report_path}\")\n    print(f\"  3. JSON Results: {json_path}\")\n    print(f\"  4. Visualizations: {plots_dir}/\")\n    print(\"\\n[NOTE] All files are saved in:\")\n    print(f\"  - Results: {OUTPUT_DIR}\")\n    print(f\"  - Reports: {PATHS['reports_dir']}\")\nelse:\n    print(\"[INFO] No evaluation was performed.\")\n    print(\"[INFO] Please update Cell 4 with your model IDs and re-run the notebook.\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "lactech-stt",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}